<!-- Produced by version 3.14 (11/22/96) of SGI Frame/SGML translator -->
<CHAPTER LBL="11"><TITLE><XREFTARGET ID="41755"><XREFTARGET ID="90508">Statement-Level Parallelism</TITLE><PARAGRAPH>You can use statement-level parallelism in three language packages: Fortran 77, Fortran&nbsp;90, and C. This execution model is unique in that you begin with a normal, serial program, and you can always return the program to serial execution by recompiling. Every other parallel model requires you to plan and write a parallel program from the start.&space;<INDEXTARGET ID="TIP_4b.mpcstmt1"><!-- POSTPROCESSDATA: TIP_4b.mpcstmt1|parallel computation:statement-level --></PARAGRAPH>
<SECTION1 LBL="" HELPID = ""><TITLE>Products for Statement-Level Parallelism</TITLE><PARAGRAPH>Software support for statement-level parallelism is available from Silicon Graphics and from independent vendors.</PARAGRAPH>
<SECTION2 LBL="" HELPID = ""><TITLE>Silicon Graphics Support</TITLE><PARAGRAPH>The parallel features of the three languages from Silicon Graphics are documented in detail in the manuals listed in <XREF IDREF="84869" TYPE="TABLE">Table&nbsp;11-1</XREF>. </PARAGRAPH>
<TABLE COLUMNS="3"><CAPTION LBL="11-1"><PREFIX>Table 11-1 </PREFIX><XREFTARGET ID="84869">Documentation for Statement-Level Parallel Products</CAPTION>
<TABLEHEADING><CELL LEFT="0" WIDTH="108"><PARAGRAPH>&lbreak;Manual</PARAGRAPH>
</CELL>
<CELL LEFT="115" WIDTH="64"><PARAGRAPH>Document 
Number</PARAGRAPH>
</CELL>
<CELL LEFT="185" WIDTH="216"><PARAGRAPH>&lbreak;Contents</PARAGRAPH>
</CELL>
</TABLEHEADING>
<TABLEBODY><ROW><CELL LEFT="0" WIDTH="108"><PARAGRAPH><DOCTITLE>C Language Reference 
Manual</DOCTITLE></PARAGRAPH>
</CELL>
<CELL LEFT="115" WIDTH="64"><PARAGRAPH>007-0701<ITALICS>-nnn</ITALICS>&space;</PARAGRAPH>
</CELL>
<CELL LEFT="185" WIDTH="216"><PARAGRAPH>Covers all pragmas, including parallel ones.</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="108"><PARAGRAPH>IRIS Power C User's Guide</PARAGRAPH>
</CELL>
<CELL LEFT="115" WIDTH="64"><PARAGRAPH>007-0702-<ITALICS>nnn</ITALICS>&space;</PARAGRAPH>
</CELL>
<CELL LEFT="185" WIDTH="216"><PARAGRAPH>Use of Power C source analyzer to place pragmas 
automatically.</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="108"><PARAGRAPH><DOCTITLE>MIPSpro Fortran 77 
Programmer's Guide </DOCTITLE></PARAGRAPH>
</CELL>
<CELL LEFT="115" WIDTH="64"><PARAGRAPH>007-2361<ITALICS>-nnn</ITALICS>&space;</PARAGRAPH>
</CELL>
<CELL LEFT="185" WIDTH="216"><PARAGRAPH>General use of Fortran 77, including parallelizing 
assertions and directives.</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="108"><PARAGRAPH><DOCTITLE>MIPSpro Power Fortran 77 
Programmer's Guide</DOCTITLE></PARAGRAPH>
</CELL>
<CELL LEFT="115" WIDTH="64"><PARAGRAPH>007-2363<ITALICS>-nnn</ITALICS>&space;</PARAGRAPH>
</CELL>
<CELL LEFT="185" WIDTH="216"><PARAGRAPH>Use of the Power Fortran source analyzer to place 
directives automatically.</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="108"><PARAGRAPH><DOCTITLE>MIPSpro Fortran 90 
Programmer's Guide</DOCTITLE></PARAGRAPH>
</CELL>
<CELL LEFT="115" WIDTH="64"><PARAGRAPH>007-2761<ITALICS>-nnn</ITALICS>&space;</PARAGRAPH>
</CELL>
<CELL LEFT="185" WIDTH="216"><PARAGRAPH>General use of Fortran 90, including parallelizing 
assertions and directives.</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="108"><PARAGRAPH><DOCTITLE>MIPSpro Power Fortran 90 
Programmer's Guide</DOCTITLE></PARAGRAPH>
</CELL>
<CELL LEFT="115" WIDTH="64"><PARAGRAPH>007-2760<ITALICS>-nnn</ITALICS>&space;</PARAGRAPH>
</CELL>
<CELL LEFT="185" WIDTH="216"><PARAGRAPH>Use of the Power Fortran 90 source analyzer to place 
directives automatically.</PARAGRAPH>
</CELL>
</ROW>
</TABLEBODY>
</TABLE>
</SECTION2>
<SECTION2 LBL="" HELPID = ""><TITLE>Products from Other Vendors</TITLE><PARAGRAPH>In addition to these products from Silicon Graphics, the High Performance Fortran (HPF) compiler from the Portland Group is a compiler for Fortran 90 augmented to the HPF standard. It supports automatic parallelization. (Refer to <LAUNCHWORD APP="/usr/sbin/nr" PARMS="http://www.pgroup.com">http://www.pgroup.com</LAUNCHWORD>
 for more information).</PARAGRAPH>
<PARAGRAPH>The FORGE products from Applied Parallel Research (APRI) contain a Fortran 77 source analyzer that can insert parallelizing directives, although not the directives supported by MIPSpro Fortran 77. (Refer to <LAUNCHWORD APP="/usr/sbin/nr" PARMS="http://www.apri.com">http://www.apri.com</LAUNCHWORD>
 for more information.)</PARAGRAPH>
</SECTION2>
</SECTION1>
<SECTION1 LBL="" HELPID = ""><TITLE>Creating Parallel Programs</TITLE><PARAGRAPH>In each of the three languages, the language compiler supports explicit statements that command parallel execution (<KEYWORD>#pragma</KEYWORD> lines for C; directives and assertions for Fortran). However, placing these statements can be a demanding, error-prone task. It is easy to create a suboptimal program, or worse, a program that is incorrect in subtle ways. Furthermore, small changes in program logic can invalidate parallel directives in ways that are hard to foresee, so it is difficult to modify a program that has been manually made parallel.</PARAGRAPH>
<PARAGRAPH>For each language, there is a source-level program analyzer that is sold as a separate product (IRIS POWER C, MIPSpro Power Fortran 77, MIPSpro Power Fortran 90). The analyzer identifies sections of the program that can safely be executed in parallel, and automatically inserts the parallelizing directives. After any logic change, you can run the analysis again, so that maintenance is easier.</PARAGRAPH>
<PARAGRAPH>The source analyzer makes conservative assumptions about the way the program uses data. As a result, it often is unable to find all the potential parallelism. However, the analyzer produces a detailed listing of the program source, showing each segment that could or could not be parallelized, and why. Directed by this listing, you insert source assertions that give the analyzer more information about the program.</PARAGRAPH>
<PARAGRAPH>The method of creating an optimized parallel program is as follows:</PARAGRAPH>
<ORDEREDLIST><LIST><PARAGRAPH>Write a complete application that runs on a single processor.</PARAGRAPH>
</LIST>
<LIST><PARAGRAPH>Completely debug and verify the correctness of the program in serial execution.</PARAGRAPH>
</LIST>
<LIST><PARAGRAPH>Apply the source analyzer and study the listing it produces.</PARAGRAPH>
</LIST>
<LIST><PARAGRAPH>Add assertions to the source program. These are not explicit commands to parallelize, but high-level statements that describe the program's use of data.</PARAGRAPH>
</LIST>
<LIST><PARAGRAPH>Repeat steps 3 and 4 until the analyzer finds as much parallelism as possible.</PARAGRAPH>
</LIST>
<LIST><PARAGRAPH>Run the program on a single-memory multiprocessor.</PARAGRAPH>
</LIST>
</ORDEREDLIST>
<PARAGRAPH>When the program requires maintenance, you make the necessary logic changes and, simultaneously, remove any assertions about the changed code&mdash;unless you are certain that the assertions are still true of the modified logic. Then repeat the preceding procedure from step 2.</PARAGRAPH>
</SECTION1>
<SECTION1 LBL="" HELPID = ""><TITLE><XREFTARGET ID="66886">Managing Statement-Parallel Execution</TITLE><PARAGRAPH>The run-time library for all three languages is the same, <FILENAME>libmp</FILENAME>. It is documented in the <REFPAGE>mp(3)</REFPAGE> reference page. <FILENAME>libmp</FILENAME> uses IRIX lightweight processes to implement parallel execution (see <XREF IDREF="43999" TYPE="TITLE">Chapter&nbsp;12, &ldquo;Process-Level Parallelism&rdquo;</XREF>).</PARAGRAPH>
<PARAGRAPH>When a parallel program starts, the run-time support creates a pool of lightweight processes using the <FUNCTION>sproc()</FUNCTION> function. Initially the extra processes are blocked, while one process executes the opening passage of the program. When execution reaches a parallel section, the run-time library code unblocks as many processes as necessary. Each process begins to execute the same block of statements. The processes share global variables, while each allocates its own copy of variables that are local to one iteration of a loop, such as a loop index.</PARAGRAPH>
<PARAGRAPH>When a process completes its portion of the work of that parallel section, it returns to the run-time library code, where it picks up another portion of work if any work remains, or suspends until the next time it is needed. At the end of the parallel section, all extra processes are suspended and the original process continues to execute the serial code following the parallel section.</PARAGRAPH>
<SECTION2 LBL="" HELPID = ""><TITLE><XREFTARGET ID="57205">Controlling the Degree of Parallelism</TITLE><PARAGRAPH>You can specify the number of lightweight processes that are started by a program. In IRIS POWER C, you can use <COMMAND>#pragma numthreads</COMMAND> to specify the exact number of processes to start, but it is not a good idea to embed this number in a source program. In all implementations, the run-time library by default starts enough processes so there is one for each CPU in the system. That default is often too high, since typically not all CPUs are available for one program.</PARAGRAPH>
<PARAGRAPH>The run-time library checks an environment variable, MP_SET_NUM_THREADS, for the number of processes to start. You can use this environment variable to choose the number of processes used by a particular run of the program, thereby tuning the program's requirements to the system load. You can even force a parallelized program to execute on a single CPU when necessary.</PARAGRAPH>
<PARAGRAPH>MIPSpro Fortran 77 and MIPSpro Fortran 90 also recognize additional environment variables that specify a range of process numbers, and use more or fewer processes within this range as system load varies. (See the <DOCTITLE>Programmer's Guide</DOCTITLE> for the language for details.)</PARAGRAPH>
<PARAGRAPH>At certain points the multiple processes must wait for one another before continuing. They do this by waiting in a busy loop for a certain length of time, then by blocking until they are signaled. You can specify the amount of time that a process should spend spinning before it blocks, using either source directives or an environment variable (see the <DOCTITLE>Programmer's Guide</DOCTITLE> for the language for system functions for this purpose).</PARAGRAPH>
</SECTION2>
<SECTION2 LBL="" HELPID = ""><TITLE>Choosing the Loop Schedule Type</TITLE><PARAGRAPH>Most parallel sections are loops. The benefit of parallelization is that some iterations of the loop are executed in one CPU, concurrent with other iterations of the same loop in other CPUs. But how are the different iterations distributed across processes? The languages support four possible methods of scheduling loop iterations, as summarized in <XREF IDREF="75650" TYPE="TABLE">Table&nbsp;11-2</XREF>.</PARAGRAPH>
<TABLE COLUMNS="2"><CAPTION LBL="11-2"><PREFIX>Table 11-2 </PREFIX>&space;<EMPHASIS>(continued)        </EMPHASIS><XREFTARGET ID="75650">Loop Scheduling Types</CAPTION>
<TABLEHEADING><CELL LEFT="0" WIDTH="72"><PARAGRAPH>Schedule</PARAGRAPH>
</CELL>
<CELL LEFT="80" WIDTH="315"><PARAGRAPH>Purpose </PARAGRAPH>
</CELL>
</TABLEHEADING>
<TABLEBODY><ROW><CELL LEFT="0" WIDTH="72"><PARAGRAPH>SIMPLE</PARAGRAPH>
</CELL>
<CELL LEFT="80" WIDTH="315"><PARAGRAPH>Each process executes &bracketleftbt;<VARIABLE></VARIABLE><VARIABLE>N</VARIABLE>/<VARIABLE>P&bracketrightbt iterations starting at Q</VARIABLE>*(&bracketleftbt;<VARIABLE></VARIABLE><VARIABLE>N</VARIABLE>/<VARIABLE>P&bracketrightbt). First process to 
finish takes the remainder chunk, if any.</VARIABLE>
</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="72"><PARAGRAPH>DYNAMIC</PARAGRAPH>
</CELL>
<CELL LEFT="80" WIDTH="315"><PARAGRAPH>Each process executes <VARIABLE>C</VARIABLE> iterations of the loop, starting with the next undone 
chunk unit, returning for another chunk until none are left undone.</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="72"><PARAGRAPH>INTERLEAVE</PARAGRAPH>
</CELL>
<CELL LEFT="80" WIDTH="315"><PARAGRAPH>Each process executes <VARIABLE>C</VARIABLE> iterations at <VARIABLE>C</VARIABLE>*<VARIABLE>Q</VARIABLE>, <VARIABLE>C</VARIABLE>*2<VARIABLE>Q</VARIABLE>, <VARIABLE>C</VARIABLE>*3<VARIABLE>Q</VARIABLE>...</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="72"><PARAGRAPH>GSS</PARAGRAPH>
</CELL>
<CELL LEFT="80" WIDTH="315"><PARAGRAPH>Each process executes chunks of decreasing size, (<VARIABLE>N</VARIABLE>/2<VARIABLE>P</VARIABLE>), (<VARIABLE>N</VARIABLE>/4<VARIABLE>P</VARIABLE>),...</PARAGRAPH>
</CELL>
</ROW>
</TABLEBODY>
</TABLE>
<PARAGRAPH>The variables used in <XREF IDREF="75650" TYPE="TABLE">Table&nbsp;11-2</XREF> are as follows: </PARAGRAPH>
<TABLE COLUMNS="2"><TABLEBODY><ROW><CELL LEFT="0" WIDTH="49"><PARAGRAPH><VARIABLE>N</VARIABLE>&space;</PARAGRAPH>
</CELL>
<CELL LEFT="55" WIDTH="337"><PARAGRAPH>Number of iterations in the loop, determined from the source or at run-time.</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="49"><PARAGRAPH><VARIABLE>P</VARIABLE>&space;</PARAGRAPH>
</CELL>
<CELL LEFT="55" WIDTH="337"><PARAGRAPH>Number of available processes, set by default or by environment variable 
(see <XREF IDREF="57205" TYPE="TITLE">&ldquo;Controlling the Degree of Parallelism&rdquo;</XREF>).</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="49"><PARAGRAPH><VARIABLE>Q</VARIABLE>&space;</PARAGRAPH>
</CELL>
<CELL LEFT="55" WIDTH="337"><PARAGRAPH>Number of a process, from 0 to <VARIABLE>N</VARIABLE>-1.</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="49"><PARAGRAPH><VARIABLE>C</VARIABLE>&space;</PARAGRAPH>
</CELL>
<CELL LEFT="55" WIDTH="337"><PARAGRAPH>&ldquo;Chunk&rdquo; size, set by directive or by environment variable.</PARAGRAPH>
</CELL>
</ROW>
</TABLEBODY>
</TABLE>
<PARAGRAPH>The effects of the scheduling types depend on the nature of the loops being parallelized. For example:</PARAGRAPH>
<BULLETLIST><BULLET><PARAGRAPH>The SIMPLE method works well when <VARIABLE>N</VARIABLE> is relatively small. However, unless <VARIABLE>N</VARIABLE> is evenly divided by <VARIABLE>P</VARIABLE>, there will be a time at the end of the loop when fewer than <VARIABLE>P</VARIABLE> processes are working, and possibly only one.</PARAGRAPH>
</BULLET>
<BULLET><PARAGRAPH>The DYNAMIC and INTERLEAVE methods allow you to set the chunk size to control the span of an array referenced by each process. You can use this to reduce cache effects. When <VARIABLE>N</VARIABLE> is very large so that not all data fits in memory, INTERLEAVE may reduce the amount of paging compared to DYNAMIC.</PARAGRAPH>
</BULLET>
<BULLET><PARAGRAPH>The guided self-scheduling (GSS) method is good for triangular matrices and other algorithms where loop iterations become faster toward the end.</PARAGRAPH>
</BULLET>
</BULLETLIST>
<PARAGRAPH>You can use source directives or pragmas within the program to specify the scheduling type and chunk size for particular loops. Where you do not specify the scheduling, the run-time library uses a default method and chunk size. You can establish this default scheduling type and chunk size using environment variables.</PARAGRAPH>
</SECTION2>
<SECTION2 LBL="" HELPID = ""><TITLE>Distributing Data</TITLE><PARAGRAPH>In any statement-level parallel program, memory cache contention can harm performance. This subject is covered under <XREF IDREF="51657" TYPE="TITLE">&ldquo;Dealing With Cache Contention&rdquo;</XREF>.</PARAGRAPH>
<PARAGRAPH>When a statement-parallel program runs in an Origin2000 or Onyx2 system, the location of the program's data can affect performance. These issues are covered at length under <XREF IDREF="88017" TYPE="TITLE">&ldquo;Using Origin2000 Nonuniform Memory&rdquo;</XREF>.</PARAGRAPH>
<PARAGRAPH>&space;</PARAGRAPH>
</SECTION2>
</SECTION1>
</CHAPTER>
