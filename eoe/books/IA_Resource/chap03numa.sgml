<!-- Fragment document type declaration subset:
ArborText, Inc., 1988-1996, v.4001
<!DOCTYPE SGIDOCBK PUBLIC "-//Silicon Graphics, Inc.//DTD DocBook V2.3-based Subset V1.5//EN" [
<!ENTITY a11342.drw SYSTEM "a11342.drw" NDATA drw>
<!ENTITY a11934 SYSTEM "online/a11934.gif" NDATA gif>
<!ENTITY a11933 SYSTEM "online/a11933.gif" NDATA gif>
<!ENTITY a11932 SYSTEM "online/a11932.gif" NDATA gif>
<!ENTITY a11931 SYSTEM "online/a11931.gif" NDATA gif>
<!ENTITY a11930 SYSTEM "online/a11930.gif" NDATA gif>
<!ENTITY a11929 SYSTEM "online/a11929.gif" NDATA gif>
<!ENTITY a11928 SYSTEM "online/a11928.gif" NDATA gif>
<!ENTITY a11927 SYSTEM "online/a11927.gif" NDATA gif>
<!ENTITY a11926 SYSTEM "online/a11926.gif" NDATA gif>
<!ENTITY a11981 SYSTEM "online/a11981.gif" NDATA gif>
<!ENTITY AppA.sgml SYSTEM "AppA.sgml">
<!ENTITY a11342 SYSTEM "a11342" NDATA eps>
<!ENTITY a11342.eps SYSTEM "a11342.eps" NDATA eps>
<!ENTITY chap06limit.sgml SYSTEM "chap06limit.sgml">
<!ENTITY fig9 SYSTEM "online/fig9.gif" NDATA gif>
<!ENTITY fig8 SYSTEM "online/fig8.gif" NDATA gif>
<!ENTITY fig7 SYSTEM "online/fig7.gif" NDATA gif>
<!ENTITY fig6 SYSTEM "online/fig6.gif" NDATA gif>
<!ENTITY fig5 SYSTEM "online/fig5.gif" NDATA gif>
<!ENTITY fig4 SYSTEM "online/fig4.gif" NDATA gif>
<!ENTITY fig3 SYSTEM "online/fig3.gif" NDATA gif>
<!ENTITY fig2 SYSTEM "online/fig2.gif" NDATA gif>
<!ENTITY fig1 SYSTEM "online/fig1.gif" NDATA gif>
<!ENTITY a10111.gif SYSTEM "a10111.gif" NDATA gif>
<!ENTITY a10112.gif SYSTEM "a10112.gif" NDATA gif>
<!ENTITY a11292.gif SYSTEM "a11292.gif" NDATA gif>
<!ENTITY a11293.gif SYSTEM "a11293.gif" NDATA gif>
<!ENTITY a11294.gif SYSTEM "a11294.gif" NDATA gif>
<!ENTITY a11295.gif SYSTEM "a11295.gif" NDATA gif>
<!ENTITY a11296.gif SYSTEM "a11296.gif" NDATA gif>
<!ENTITY a11297.gif SYSTEM "a11297.gif" NDATA gif>
<!ENTITY chap05acct.sgml SYSTEM "chap05acct.sgml">
<!ENTITY chap04miser.sgml SYSTEM "chap04miser.sgml">
<!ENTITY chap02sched.sgml SYSTEM "chap02sched.sgml">
<!ENTITY chap01intro.sgml SYSTEM "chap01intro.sgml">
]>
-->
<chapter>
<title>Memory Management</title>
<para>This chapter provides an overview of managing memory on an IRIX system. For information on NUMA kernel tunable parameters, see Appendix A of the <citetitle>IRIX Admin: System Configuration and Operation</citetitle> manual.</para>
<para>This chapter is divided into the following sections: <itemizedlist>
<listitem><para></para>
</listitem>
<listitem><para></para>
</listitem></itemizedlist></para>
<para><comment>(Need to flush out the needed content for sys admins. The following sections are guesses as to what might belong here.)</comment></para>
<section>
<title>(Notes)</title>
<para>Origin2000 distributed-shared memory is located in a single shared address space but is physically dispersed amongst several nodes. </para>
<para>Former systems had memory centrally located and only accessible over a single shared bus. </para>
<para>The interconnection fabric is a mesh of multiple point-to-point links connected by the routing switches. These links and switches allow multiple memory accesses to occur simultaneously. </para>
<para>To a processor, main memory appears as a single addressable space containing many blocks or pages. </para>
<para>Page migration hardware moves data into memory closer to a processor that frequently uses it to reduce memory latency.</para>
<para>The memory hierarchy (in order of increasing memory latency) is as follows: processor registers, cache (primary and secondary), home memory (local and remote), and remote caches.</para>
<para></para>
</section>
<section>
<title>Overview??</title>
<para><comment>(The following info was taken from &ldquo;Uncle Art's Big Book of IRIX&rdquo;. The info may be outdated or not suited for customers. Please review carefully.)</comment></para>
<para>Memory on a multiuser computer is a limited resource. An operating system such as IRIX cannot maintain every process in main memory at all times. To do so would require an unreasonable amount of memory. Therefore, the available memory must be managed to allow the greatest number of processes to run with the greatest efficiency. </para>
<para>The IRIX system controls many processes by placing portions of text and data in memory as they are required, possible removing them to secondary storage when they are no longer needed. Some programs, such as the kernel itself, is always in memory. Other programs are placed, in whole or part, in memory as needed. The algorithm by which the IRIX system places programs in memory is called <firstterm>paging</firstterm> or <firstterm>page swapping</firstterm>. </para>
<para>The various regions that make up a process are divided into <firstterm>pages</firstterm> of 4Kbytes <comment>(is this still true???)</comment>. Each page can be independently read into memory or written out to secondary storage (the <firstterm>swap device</firstterm>). Pages are read into memory only when needed, so that if only ten percent of a program's code is executed, only ten percent need be read into memory. This policy is referred to as <firstterm>demand paging</firstterm>. Demand paging systems have obvious advantages over <firstterm>swapping</firstterm> systems, which must read an entire program into memory before executing it. Demand paging systems are not only more efficient in their use of resources, they actually allow the user to run programs that are larger than physical
memory.</para>
<para>The IRIX hardware provides mechanisms for mapping virtual pages to physical addresses and controlling access to physical memory. The hardware maintains a cache of virtual to physical address mappings, called the <firstterm>translation look-aside buffer</firstterm>, or TLB. For the most part, the TLB entries are loaded from user page tables. These entries are transient and may be replaced at any time (when a new entry is loaded, it is dropped into a random slot in the TLB, replacing an old entry). A few TLB entries are semi-permanent, or ``wired.'' These entries stay in the TLB until explicitly removed and are used by the kernel for mapping certain pages into virtual memory (for example, the uthread of the running process is ``wired'' into a certain virtual address).</para>
</section>
<section>
<title>Memory Concepts</title>
<para>The following concepts are related to memory on an IRIX system: <deflist id="Z898531418janelle">
<deflistentry>
<term>Cache coherency</term>
<listitem><para></para>
</listitem></deflistentry>
<deflistentry>
<term>Distributed memory</term>
<listitem><para></para>
</listitem></deflistentry>
<deflistentry>
<term>Single shared address space</term>
<listitem><para></para>
</listitem></deflistentry>
<deflistentry>
<term>Memory latency</term>
<listitem><para></para>
</listitem></deflistentry>
<deflistentry>
<term>Main memory</term>
<listitem><para></para>
</listitem></deflistentry>
<deflistentry>
<term>NUMA</term>
<listitem><para></para>
</listitem></deflistentry>
<deflistentry>
<term>Paging</term>
<listitem><para></para>
</listitem></deflistentry>
<deflistentry>
<term>Migration</term>
<listitem><para></para>
</listitem></deflistentry>
</deflist><comment>Others????</comment></para>
</section>
<section>
<title>System's Memory Structure</title>
<para></para>
</section>
<section>
<title>Page Cache</title>
<para></para>
</section>
<section>
<title>Swap Space Management</title>
<para></para>
</section>
<section>
<title>Translation Lookaside Buffer (TLB)</title>
<para></para>
</section>
<section>
<title>Virtual Memory</title>
<para>Virtual memory (VM), or virtual addressing, is used to divide the system's relatively small amount of physical memory among the potentially larger amount of logical processes in a program. It does this by dividing physical memory into pages, and then allocating pages to processes as the pages are needed. In other words, instead of a large, global, highly contended page cache, IRIX has one small page cache per memory object.</para>
</section>
<section>
<title>Memory Placement</title>
<para></para>
</section>
<section>
<title>Process Swapping</title>
<para>Under some conditions, the demand for memory will become great enough that is worthwhile to swap out entire processes. The memory scheduler, <literal>sched</literal>, is responsible for these transactions. When memory is needed, <literal>sched</literal> finds processes that are sleeping and swaps out their user blocks, kernel stacks, and page tables. The memory scheduler swaps processes back in when memory becomes available. If memory continues to be congested, the memory scheduler makes sure that processes do not spend too much time swapped out, by swapping more sleeping processes out, and swapping processes back in on a first-in, first-out basis.</para>
</section>
<section>
<title>Migration</title>
<section>
<title>Dynamic Memory Replication</title>
<para>When the shared memory page is read-only, the hardware detects the sharing, and the memory management subsystem in IRIX intervenes to either replicate the specific memory page being shared (by placing a copy in the memory location closet to each of the distributed processors) or identify an existing nearby copy that the application can reference.</para>
</section>
<section>
<title>Dynamic Memory Migration</title>
<para>When a single shared memory page is being accessed, counters in the hardware detect the off-node references. When the number or proportion of the references passes some pre-defined relative or absolute threshold, the IRIX memory management subsystem migrates the common individual memory page close to the other sharing process for better locality. This increases the likelihood that related pages and threads will also migrate to maximize the performance of all the sharing threads. </para>
</section>
</section>
<section>
<title>Paging</title>
<para></para><?Pub Caret>
</section>
<section>
<title>NUMA</title>
<para>Cache coherency is supported by a distributed hardware directory.</para>
<para>Memory is distributed throughout the entire system.</para>
<para>Single shared address space for all processors.</para>
<para>Accessing memory near to a processor takes less time than accessing remote memory.</para>
<para>Main memory is available to all processors.</para>
</section>
<section>
<title>Process to Memory Affinity</title>
<para>In the IRIX system, a process is run on any available CPU. Memory is allocated in the node where the process is running, whenever possible. If this is not possible, memory is allocated as close as possible to the requesting CPU. As a process executes on a processor, it causes more and more of its data and instruction text to be loaded into processor cache and contiguous main memory. This creates an affinity between the running process and that CPU: no other process can use that CPU as effectively, and the process cannot execute as fast on any other CPU. The scheduler on an IRIX multiprocessor automatically notes the CPU on which a process last ran, and attempts to run a process on that same CPU based on the assumption that some of its data remains in cache and possibly local memory on
that CPU. <comment>(Refer to Scheduling chapter for additional info.)</comment></para>
</section>
<section>
<title>Related Commands</title>
<para>The following commands are useful for memory management: <deflist id="Z896975456janelle">
<deflistentry>
<term><command sectionref="1">dplace</command></term>
<listitem><para>Uses the placement data from <command>dprof</command> to invoke initial placement on the hardware topology.</para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="1">dlook</command></term>
<listitem><para></para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="1">dprof</command></term>
<listitem><para>Analyzes thread virtual memory access patterns to determine the best initial placement for data and threads.</para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="1">gr_sn</command></term>
<listitem><para></para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="1">gr_nstats</command></term>
<listitem><para></para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="1">nstats</command></term>
<listitem><para></para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="1">sn</command></term>
<listitem><para></para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="1">topology</command></term>
<listitem><para></para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="5">migration</command></term>
<listitem><para></para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="5">mmci</command></term>
<listitem><para></para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="5">numa</command></term>
<listitem><para></para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="5">replication</command></term>
<listitem><para></para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="5">refcnt</command></term>
<listitem><para></para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="1">mpadmin</command></term>
<listitem><para></para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="1">perfex</command></term>
<listitem><para></para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="5">r10k_counters</command></term>
<listitem><para></para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="1">time</command></term>
<listitem><para></para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="1">timex</command></term>
<listitem><para></para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="3">mp</command></term>
<listitem><para></para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="1">gmemusage</command></term>
<listitem><para></para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="1">dview</command></term>
<listitem><para>Shows memory access and placement information.</para>
</listitem></deflistentry>
</deflist></para>
<para><comment>(Others??)</comment></para>
</section>
</chapter>
<?Pub *0000014318>
