<!-- Fragment document type declaration subset:
ArborText, Inc., 1988-1996, v.4001
<!DOCTYPE SGIDOCBK PUBLIC "-//Silicon Graphics, Inc.//DTD DocBook V2.3-based Subset V1.5//EN" [
<!ENTITY a11981 SYSTEM "online/a11981.gif" NDATA gif>
]>
-->
<?Pub Inc>
<chapter>
<title>IRIX Scheduling</title>
<para>This chapter provides an overview of process and job scheduling on an
IRIX system. For general information on managing user processes, see the <citetitle>
IRIX Admin: System Configuration and Operation</citetitle> manual. For information
on controlling the division of system resources among individual users or
arbitrary groups of users, refer to the <citetitle>SHARE II for IRIX&trade;
Administrator's Guide</citetitle>.</para>
<para>This chapter is divided into the following sections: <itemizedlist>
<listitem><para>Scheduling Overview, <xref linkend="Z899899332janelle"></para>
</listitem>
<listitem><para>Scheduling Concepts, <xref linkend="Z899899349janelle"></para>
</listitem>
<listitem><para>Earnings-Based Scheduler, <xref linkend="Z899899396janelle"></para>
</listitem>
<listitem><para>Process States, <xref linkend="Z899899515janelle"></para>
</listitem>
<listitem><para>Scheduling Queues, <xref linkend="Z899899528janelle"></para>
</listitem>
<listitem><para>Types of Jobs, <xref linkend="Z899899543janelle"></para>
</listitem>
<listitem><para>Scheduling Priorities, <xref linkend="Z899899561janelle"></para>
</listitem>
<listitem><para>Affinity Scheduling, <xref linkend="Z899899579janelle"></para>
</listitem>
<listitem><para>Multithreaded Process Scheduling, <xref linkend="Z899899602janelle"></para>
</listitem>
<listitem><para>Synchronized Execution, <xref linkend="Z899899622janelle"></para>
</listitem>
<listitem><para>Coordinating Interactive and Batch Jobs, <xref linkend="Z899899634janelle"></para>
</listitem>
<listitem><para>Scheduling Policies, <xref linkend="Z899899653janelle"></para>
</listitem>
<listitem><para>Related Commands, <xref linkend="Z899899667janelle"></para>
</listitem></itemizedlist></para>
<para></para>
<section id="Z899899332janelle">
<title>Scheduling Overview</title>
<para>Effectively scheduling processes and jobs on a computer system is a
main concern for system administrators. Scheduling on larger computer systems
involves not only deciding when a process will run, but also where it will
run. Therefore, scheduling can be defined as the decision of which process
to execute on a certain processing node at a certain time. </para>
<para>When scheduling processes, system administrators try to effectively
manage system resources such as CPUs, memory, and barriers. <comment>(Need
more info on how to manage these; do we need to define/discuss barriers?)
</comment>  </para>
<para><comment>(Need more info here.)</comment></para>
<para>The following sections provide more information on IRIX scheduling.
</para>
</section>
<section id="Z899899349janelle">
<title>Scheduling Concepts</title>
<para>Basic scheduling concepts related to IRIX scheduling are described below. 
</para>
<para> <comment>ARE THERE OTHER CONCEPTS THAT NEED TO BE DEFINED??</comment></para>
<para><deflist id="Z896709636janelle">
<deflistentry>
<term>Process</term>
<listitem><para>A process is a collection of resources within the computer
system. A process is responsible for managing itself and its resources. This
is accomplished by splitting each process into two pieces: the kernel portion
and the user portion. </para>
<para>While in the user portion, the process may only execute unprivileged
code; to access a system resource the process executes a trap and enters the
kernel portion of the process. This trap mechanism ensures that the process
always executes kernel code as a trusted entity. The kernel portion of a process
may access all parts of kernel memory.</para>
<para>A process is said to be restricted if it can only run on a specific
processor.</para>
</listitem></deflistentry>
<deflistentry>
<term>Processor</term>
<listitem><para>A processor is the basic scheduling entity on the system;
there may be any number of processors on the system. Each processor is responsible
for its own scheduling decisions based on information kept in a global information
structure called the <firstterm>runqueue</firstterm>.</para>
<para>A processor is said to be restricted if it can only run processes that
are restricted to run on it.</para>
</listitem></deflistentry>
<deflistentry>
<term>Tick Interrupts</term>
<listitem><para>In normal operation, the kernel pauses to make scheduling
decisions every 10 milliseconds in every CPU. The duration of this interval,
which is called the <firstterm>tick</firstterm> because it is the metronomic
beat of the scheduler, is defined in <filename>sys/param.h</filename>. Every
CPU is normally interrupted by a timer every tick interval. (However, the
CPUs in a multiprocessor are not necessarily synchronized. Different CPUs
may take tick interrupts at a different times.)  </para>
<para>During the tick interrupt, the kernel updates accounting values, does
other housekeeping work, and chooses which process to run next (usually this
is the interrupted process, unless a process of higher priority has become
ready to run). The tick interrupt is the mechanism that makes IRIX scheduling <firstterm>
preemptive</firstterm>; that is, it is the mechanism that allows a high-priority
process to take a CPU away from a lower-priority process.  Before the kernel
returns to the chosen process, it checks for pending signals, and may divert
the process into a signal handler.</para>
</listitem></deflistentry>
<deflistentry>
<term>Time Slices</term>
<listitem><para>Each process has a guaranteed time slice, which is the amount
of time it is normally allowed to execute without being preempted. By default
the time slice is 10 ticks, or 100 ms, on a multiprocessor system and 2 ticks,
or 20 ms, on a uniprocessor system. A typical process is usually blocked for
I/O before it reaches the end of its time slice.  At the end of a time slice,
the kernel chooses which process to run next on the same CPU based on process
priorities. When runnable processes have the same priority, the kernel runs
them in turn. </para>
</listitem></deflistentry>
<deflistentry>
<term>Processor Set</term>
<listitem><para><comment>(Need info)</comment></para>
</listitem></deflistentry>
<deflistentry>
<term>Migration</term>
<listitem><para><comment>(Need info)</comment></para>
</listitem></deflistentry>
<deflistentry>
<term>Context Switch</term>
<listitem><para>A transition from one process state to another. When a context
switch occurs, all process state information is saved so the process can continue
later from the state it was in when it switched out.</para>
</listitem></deflistentry>
<deflistentry>
<term>Preemption</term>
<listitem><para><comment>(Need info)</comment></para>
</listitem></deflistentry>
<deflistentry>
<term>Priority</term>
<listitem><para><comment>(Need info&mdash;put in reference to Section 1.7.)
</comment></para>
</listitem></deflistentry>
</deflist></para>
</section>
<section id="Z899899396janelle">
<title>Earnings-Based Scheduler</title>
<para>UNIX scheduling is often based on time-sharing: a priority-based scheduling
policy where priorities degrade with increasing CPU utilization (see <xref
linkend="Z899899561janelle">, for more information on scheduling priorities).
New processes are inserted at the end of the list, and the list is completely
searched for the best candidate on every scheduling cycle. Priority-based
scheduling uses aging. However, this model is corrupted on multi-PE systems
where such factors as NUMA and interactive response need to be taken into
account. Therefore, in the current version of the IRIX system, the standard
priority-based scheduling is replaced with space-sharing earnings-based scheduling. 
</para>
<para>With space-sharing earnings-based scheduling, interactive processes
earn time into a "bank account" to gain fair entitlement to the system, where
cache affinity and memory affinity are taken into account. Time-share processes
with equal priority are selected to run based on the amount of CPU time in
their account. When a process runs, the amount of CPU time it consumes is
debited from its account.</para>
<para>The account balance is incremented by the scheduler using a space-sharing
method. If all of the waiting processes have the same nice value or priority,
the amount of available CPU time is spread evenly among all the processes
waiting on the run queue, and the account is incremented accordingly. (This
assumes none of the processes share a user ID.) If the nice values differ,
the increment is weighted by the nice value. In other words, processes with
a higher priority are given more CPU time than a process with a low priority.
</para>
<para><comment>(Do we need more info?)</comment></para>
</section>
<section id="Z899899515janelle">
<title>Process States</title>
<para>The scheduling states every process goes through are depicted in <xref
linkend="Z896891491chrisw">. </para>
<figure id="Z896891491chrisw">
<graphic entityref="a11981"></graphic>
<title>Process States</title>
</figure>
<para>The <firstterm>queued</firstterm> state indicates that the process is
waiting to run on a processor. Processes in this state are kept in the runqueue.
When a processor has finished with a previous process and is available for
work, it queries the runqueue and picks the &ldquo;best&rdquo; process to
run next (depending on such factors as process priority, the nice value, and
the amount of CPU time accumulated). The process that is picked is then <firstterm>
dispatched</firstterm> to a processor and enters the <firstterm>running</firstterm>
state.</para>
<para>The <firstterm>running</firstterm> state indicates that the process
is currently in control of a processor. The process may either block and enter
the waiting state, or it can go into the <firstterm>preemption</firstterm>
state by preempting itself (at a time slice, for instance) and entering the
queued state. In either case, the process is said to yield the processor.
</para>
<para>The <firstterm>waiting</firstterm> state indicates that the process
has been <firstterm>blocked</firstterm> waiting for some event to occur. A
process will stay in the waiting state until it is explicitly queued by some
other process. <comment>(need more info here&mdash;how is it queued by another
process? What does this mean? For example...?)</comment></para>
<para>A <firstterm>dispatch</firstterm> cycle occurs when one of the processors
in the system initiates a scan of the runqueue to find a new process to run.
During a dispatch cycle, each process on the runqueue will be checked to determine
if it is runnable. The first runnable process found is dispatched to the processor.
The runnable attribute is an instantaneous condition that indicates whether
the process can enter the running state. Restrictions that make a process
unrunnable include having a swapped user area, the current processor not in
the processor set for a process, the inability to access the graphics head,
and being locked to a different processor.</para>
<para><comment>(Do we need more info on process states?)</comment></para>
</section>
<section id="Z899899528janelle">
<title>Scheduling Queues</title>
<para>The runqueue is the central data structure that contains the main scheduling
queues. The runqueue is arranged as a link of subqueues. Each subqueue is
responsible for managing its own list of runnable processes, including addition,
deletion, and selection of a candidate process to run and implementing a queueing
policy most appropriate for the type of scheduling requested. The ordering
of the subqueues on the runqueue list determines the overall scheduling policy
of the system. </para>
<para>There are three basic types of subqueues: ordered, deadline <comment>
(Is this still available? If not, what replaced it?)</comment>, and gang.<deflist
id="Z896714164janelle">
<deflistentry>
<term>Ordered</term>
<listitem><para>An ordered subqueue is the simplest type of queue. It maintains
a single, priority ordered queue. On a multiprocessor, this type of subqueue
applies affinity checks to each process. In addition, an ordered subqueue
may be configured as a distributed queue. This queue can be configured for
kernel, real-time, time-share, and batch subqueues. The time-share and batch
subqueues work as &ldquo;catch-all&rdquo; subqueues for processes within their
priority range.</para>
</listitem></deflistentry>
<deflistentry>
<term>Deadline</term>
<listitem><para>A deadline subqueue maintains a single, time-to-deadline ordered
queue. <comment>(Need more info.)</comment></para>
</listitem></deflistentry>
<deflistentry>
<term>Gang</term>
<listitem><para>A gang subqueue maintains a queue of gangs&mdash;groups of
shared processes descended from a common parent. In principle, the members
of a gang share an address space and must physically run in parallel to achieve
good performance. This subqueue attempts to run the processes of a gang in
parallel while maintaining fairness among gangs and with other processes on
the system. This queue can be configured for gang time-share and gang batch
subqueues.</para>
</listitem></deflistentry>
</deflist></para>
</section>
<section id="Z899899543janelle">
<title>Types of Jobs</title>
<para>There are five major types of jobs which can run on an IRIX system:
timesharing (interactive), real-time, gang, batch (background), and weightless.
 </para>
<section>
<title>Time-share Jobs</title>
<para>Time-share jobs run interactively on the system. They are typically
terminal I/O driven and are associated with a login, terminal, or window session.
On a UNIX time-sharing system, the kernel allocates the CPU to a process for
a period of time, preempts the process and schedules another one when the
first process's time has expired. It then reschedules the process to continue
execution at a later time.</para>
<para>The IRIX currency-based space-sharing scheduler changes the typical
UNIX time-sharing job scheduling policy significantly, insuring long-term
fairness for all users while enabling short-term decision making for optimal
application performance. Traditional UNIX scheduling assigns a priority to
each user. Newly spawned processes inherit the same priority, which degrades
as the process runs to allow lower-priority processes an opportunity to run.
With this scheme, a single user with a relatively high priority can spawn
a number of threads with the same priority, monopolizing the system unfairly.
A user with ten processes would receive ten times the CPU time of a user with
one equivalent process and an equal priority.  </para>
<para>The IRIX scheduler makes unfair monopolization of the system much less
likely. With currency-based scheduling, the system administrator assigns each
user a proportion or share of total CPU time (<firstterm>weight</firstterm>).
Users with multiple threads or processes must indicate the relative importance
of each by allocating each a proportion of the user's total currency. For
example, assume there are two users on the system, one with a weight of 100
and the other with 50. The first user has two jobs: one urgent and one less
urgent. The second user has two jobs of equal importance. Both users would
allocate their currencies as follows and receive the appropriate share of
the system over time:  </para>
<para><literallayout>

<emphasis>User    Weight  Alloc.Job 1  Alloc.Job 2  Sys.% Job 1  Sys.% Job 2
</emphasis>
User 1   100        75           25          1/2           1/6 
User 2    50        25           25          1/6           1/6
  </literallayout>If there are more processes than processors, the processes
with higher currency are scheduled ahead of the processes with lower currency
and are charged for their usage as it occurs. Conversely, processes with lower
currency are not running, but earn CPU time by having weight, by being ready
to run, and by waiting on busy run queues. Eventually (over microseconds of
computer time), these trends interact to provide the waiting processes with
their share of system time. </para>
<para>The long-term assurance of fairness allows the scheduler to make short-term
optimizations (for example, taking advantage of cache or memory affinity),
knowing that the earnings-rate will eventually dominate and assure each user
their share of system time.  Space-sharing scheduling recognizes that it is
often more efficient to run multiple parallel jobs on fewer CPUs. Insisting
on more CPUs with space-sharing does not effect the CPU time received: a four-CPU
job receives one-quarter the time of an equal priority job with one CPU.</para>
</section>
<section>
<title>Real-time Jobs</title>
<para>A real-time process is any process that must maintain a fixed, absolute
timing relationship with an external hardware device. Usually, it does not
matter in terms of output how long it takes a process to run (although the
goal is to have processes complete as soon as possible). A real-time process
is one that is incorrect and unusable if it fails to meet its performance
requirements, and so falls out of step with the external device. Examples
of real-time applications include simulators, data collection systems, and
process control systems.  </para>
<para>Real-time scheduling means that an application receives a fixed amount
of data in a fixed length of time.  The data can be returned at any time during
the time quantum. This type of reservation is used by applications that do
only a small amount of buffering. If the application requests more data than
its rate guarantee, the system suspends the application until it falls within
the guaranteed bandwidth.  </para>
<para>Real-time applications require precise rates of forward progress, hence
real-time users need complete control over a fixed set of system resources.
The IRIX system has a set of hardware and software extensions, collectively
known as REACT, that provide support for deterministic real-time operation.
REACT lets you restrict all nondeterministic IRIX system activity to one processor
in a multiprocessor system, leaving the remaining processors under the full
control of your real-time application. UNIX services are always available,
but IRIX system daemons never preempt a real-time process. This capability
enables you to divide your workload across processors and ensure that each
process meets its deadline.</para>
<para>REACT performs the following capabilities within the IRIX system:<itemizedlist>
<listitem><para>Restricts UNIX system overhead</para>
</listitem>
<listitem><para>Allocates processors</para>
</listitem>
<listitem><para>Directs interrupts to processors</para>
</listitem>
<listitem><para>Locks memory</para>
</listitem>
<listitem><para>Assigns process priority</para>
</listitem>
<listitem><para>Controls process scheduling</para>
</listitem>
<listitem><para>Worst-case timing</para>
</listitem></itemizedlist></para>
<para>The IRIX scheduler supports both the first-in-first-out (FIFO) and round-robin
real-time scheduling policies. Both of these policies are of higher priority
than all other processes on the system. </para>
<para>The FIFO policy schedules processes according to their assigned priority
values. The highest priority process is guaranteed control of the processor
until it willingly yields the processor or blocks on a contended resource.
If there is more than one runnable highest priority process, the highest priority
process waiting the longest is granted control of the processor. A running
process is preempted when a higher priority process becomes runnable.</para>
<para>The round-robin scheduling policy schedules processes according to their
assigned priority values. The highest priority process is guaranteed control
of the processor until it willingly yields the processor, blocks on a contended
resource, or exceeds its time quantum. If there is more than one runnable
highest priority process, the highest priority process waiting the longest
is granted control of the processor. When a process exceeds its time quantum,
it yields the processor and awaits rescheduling. A running process is preempted
when a higher priority process becomes runnable. </para>
<para>For fixed frame-rate applications, you can use the REACT/Pro Frame Scheduler.
The REACT/Pro frame scheduler is a process execution manager that completely
takes over scheduling and dispatching processes on one or more CPUs. The frame
scheduler makes it easier to organize a real-time program as a set of independent
processes, cooperating to insure that activities happen in a predefined, scheduled
sequence for data sampling, visual simulation, and related applications.</para>
<para>If neither the IRIX scheduler nor the REACT/Pro frame scheduler meet
your requirements, REACT offers a choice of ways to implement interrupt-driven,
user-level scheduling. This capability enables you to develop your own scheduling
policy or to easily port an existing executive process.</para>
<para>See the <command sectionref="5">realtime</command> man page and the <citetitle>
REACT Real-Time Programmer's Guide</citetitle> for more information on real-time
jobs. </para>
</section>
<section>
<title>Gangs</title>
<para><comment>(Does this info belong here?)</comment></para>
<para>Gang scheduling provides the ability to run multiple threads synchronized
across the processing nodes. With gang scheduling, the user can schedule related
processes or threads to run as a group. Threads that communicate with each
other using locks or semaphores can be scheduled to start in parallel. The
IRIX system then attempts to schedule the related threads concurrently, provided
the parent process has earned enough CPUs. Gang scheduling helps ensure that
a thread holding a lock is scheduled in the same time interval as another
thread that is waiting on the lock, to avoid having the second thread spin
while the first thread is not running. <comment>(need a lot more info on scheduling
issues/implications with gang scheduling)</comment>  </para>
</section>
<section>
<title>Batch Jobs</title>
<para>Batch jobs run in the background of a system; there is no user interaction
while they run. Batch jobs are submitted from a queue, are typically CPU intensive,
and are not usually associated with a specific login.</para>
<para>The IRIX system only schedules a batch process when no real-time or
time-sharing band process is available to run. If a batch job needs a higher
priority than what the standard IRIX system provides, users can use the Miser
feature to utilize their machine in a fashion that provides greater importance
to specific batch jobs. Miser enables users to achieve a predictable number
of runs of a large parallel application within a particular time frame. </para>
<para>In exchange for information about how much CPU time and memory a job
requires and a desired completion time, Miser enables the kernel to allocate
processors to optimize job completion time rather than throughput. Miser requests
that cannot be satisfied because they are unreasonable or conflict with previous
requests are rejected. </para>
<para>A job submitted to Miser is initially in <firstterm>batch</firstterm>
mode, where it runs only if there are no higher priority jobs in the run queue.
Once its designated run time arrives, the Miser job is transferred to <firstterm>
batch critical</firstterm> mode, where it runs at the highest priority to
ensure that it meets its completion time. See Chapter 3, <citetitle>Miser
</citetitle>, for more information on using Miser to schedule batch jobs.
</para>
</section>
<section>
<title>Weightless Jobs</title>
<para>A weightless job will only run when no other process is eligible to
run on the system. <comment>For example, .... </comment> A nice value of 39
results in a job becoming weightless on the system.</para>
</section>
</section>
<section id="Z899899561janelle">
<title>Scheduling Priorities</title>
<para>In normal operation, the kernel pauses briefly every few milliseconds
to make scheduling decisions for the processors under its control. Processes
are each given a guaranteed time slice, a period when each is allowed to execute
without being preempted. Often, a process yields to another process voluntarily
by making a system call, such as an I/O request, that causes it to sleep.
In this case, another process is selected to run. Otherwise, at the end of
a time slice, the kernel chooses which process to run next based on process
priority. If two runnable processes have the same priority, the kernel runs
them in turn.  The kernel chooses which interactive/time-sharing process to
run based on the currency-based scheduling policy. The chosen process then
runs on the processor with a non-degrading default priority of 20 until it
sleeps on a system call or reaches the end of its time slice.   </para>
<para>The process priority is a small integer that indicates the importance
of a process. Process priorities range in value from 0 (highest priority)
to 999 (lowest).  User processes typically begin execution with a priority
of 60. As a process gets CPU time and executes, the value of the process's
priority is gradually increased (that is, the process receives a less favorable
priority). At regular  intervals, the kernel checks to see if other processes
with better priorities are eligible to run. If there are other eligible processes,
the kernel executes a context switch and begins execution of the newly selected
process. </para>
<para>Scheduling priorities for jobs in the IRIX system range from 0 (low)
to 255 (high). Real-time processes can be assigned a priority anywhere in
the entire range, background processes (with priority 0) run when free cycles
are available, and time-sharing processes can have priorities anywhere in
the range from 0 to 40. </para>
<para>The IRIX system attaches meaning to certain priority bands, indicating
special dispatching features:<deflist id="Z896818056janelle">
<deflistentry>
<term>1&ndash;39</term>
<listitem><para>kernel-mode processes</para>
</listitem></deflistentry>
<deflistentry>
<term>30&ndash;39</term>
<listitem><para>user-mode real-time processes</para>
</listitem></deflistentry>
<deflistentry>
<term>40&ndash;127</term>
<listitem><para>general time-shared work</para>
</listitem></deflistentry>
<deflistentry>
<term>128&ndash;255</term>
<listitem><para>background processing</para>
</listitem></deflistentry>
</deflist></para>
<para>Several factors can influence the kernel's scheduling algorithm. Users
can apply a nice value in the range of 1 to 19 to their process (the default
nice value is 20). Root can also assign nice values of in the range of -1
to -20 <comment>(how do negative numbers affect priority?)</comment>. The
kernel will then add the nice value to that process's priority value whenever
comparing relative process priorities. </para>
</section>
<section id="Z899899579janelle">
<title>Affinity Scheduling</title>
<para>Affinity scheduling is a special scheduling mechanism used in multiprocessor
systems. You do not have to take action to benefit from affinity scheduling,
but you should understand the possible implications on the system.  </para>
<para>As a process executes, it causes more and more of its data and instruction
text to be loaded into the processor cache. This creates an <firstterm>affinity
</firstterm> between the process and the CPU. No other process can use that
CPU as effectively, and the process cannot execute as fast on any other CPU.
 The IRIX kernel notes the CPU on which a process last ran, and notes the
amount of the affinity between them. Affinity is measured on an arbitrary
scale.  When the process gives up the CPU&mdash;either because its time slice
is up or because it is blocked&mdash;one of three things can happen to the
CPU:<itemizedlist>
<listitem><para>The CPU runs the same process again immediately.</para>
</listitem>
<listitem><para>The CPU spins idle, waiting for work.</para>
</listitem>
<listitem><para>The CPU runs a different process.</para>
</listitem></itemizedlist></para>
<para>The first two actions do not reduce the process's affinity. But when
the CPU runs a different process, that process begins to build up an affinity
while simultaneously reducing the affinity of the earlier process.  As long
as a process has any affinity for a CPU, it is dispatched only on that CPU
if possible. When its affinity has declined to zero, the process can be dispatched
on any available CPU. </para>
<para>The result of the affinity scheduling policy is that I/O-bound processes,
which execute for short periods and build up little affinity, are quickly
dispatched whenever they become ready. CPU-bound processes, which build up
a strong affinity, are not dispatched as quickly because they have to wait
for "their" CPU to be free. However, they do not suffer the serious delays
of repeatedly "warming up" a cache.</para>
<para><comment>(Is this organized correctly, or are we essentially describing
processor cache affinity under affinity scheduling??)</comment></para>
<section>
<title>Process-to-Memory Affinity</title>
<para>Process-to-memory affinity lets the scheduler interact with the new
memory management subsystem to give short-term priority to processes and threads
whose pages are still largely memory (rather than just cache) resident.<comment>
 (need more info) </comment>  </para>
</section>
<section>
<title>Processor Cache Affinity</title>
<para>Processor cache affinity is a mechanism that attempts to take advantage
of data that a process may have fetched into a local processor cache the last
time it was running. Processor cache affinity is important: if processes are
randomly assigned to processors on each dispatch cycle, there is a high probability
that a process would spend much of its time-slice refetching data into the
cache rather than performing real work. However, affinity should not always
be applied. For example, real-time or deadline processes are more interested
in reducing latency and thus should always be dispatched immediately to any
available processor.</para>
<para>The scheduler on an IRIX-based multiprocessor automatically notes the
CPU on which a process last ran, and attempts to run a process on that same
CPU, based on the assumption that some of its data remains in cache and possibly
local memory on that CPU. The affinity mechanism implemented by the scheduler
assumes that time spent in the running state accumulates good data in the
cache, up to some limit defined by a combination between cache size and cache
miss time. This limit is called the affinity value. There is also a minimum
time (the warm time), which must be spent running to cause the affinity mechanism
to activate. This minimum is necessary because affinity management creates
additional overhead and at some point the amount of time it takes to refetch
lost cache data will be less than the time to handle cache affinity.</para>
<para>The amount of good data associated with a process in a particular cache
degrades as that processor performs other work. Depending on how much other
work has been performed, it may or may not be advantageous to force a process
to dispatch on the last processor it ran on.</para>
<para>Affinity is captured when a process yields the processor. If the process
has not run at least as long as the warm time, affinity handling is disabled
for this dispatch cycle. The affinity of the process for the processor is
capped at the affinity value and will range between the warm time and the
affinity value based on the time spent last running.</para>
<para>To estimate the effect of running other processes on cache occupancy,
each processor maintains a counter of the time spent running processes. When
a process yields a processor, a snapshot of this counter is saved. The next
dispatch decision to favor this process causes this snapshot to be compared
with the current value of the counter for that processor. If enough time has
bee spent running other tasks, the process may be dispatched anywhere. Otherwise,
the scheduler will only make it runnable for the processor it last run on.
This strategy makes affinity independent of how long a process was waiting
or queued. </para>
</section>
</section>
<section id="Z899899602janelle">
<title>Multithreaded Process Scheduling</title>
<para><comment>(Does this info belong in this chapter?)</comment></para>
<para>Multithreaded processes are created with the <command sectionref="2">
sproc</command> system call or with the <command sectionref="5">pthreads</command>
library. Each multithreaded process is accounted as a single unit for scheduling
purposes&mdash;each thread in the process is not accounted separately. </para>
<para>Using this single accounting method means that on a system in which
many processes are contending for CPU time, both single-threaded and multithreaded
processes get an equal amount of CPU time regardless of the number of threads
in the multithreaded process. This results in fair scheduling for all processes,
including those that dynamically adjust their parallelism according to the
system load (for example, Power Fortran and Power C applications that set
the <literal>OMP_DYNAMIC</literal> environment variable to <literal>TRUE</literal>). 
</para>
<para>Furthermore, the kernel scheduler does not try to schedule the threads
evenly in a multithreaded process.  The result is that contending threads
of a single multithreaded process may get unequal amounts of CPU time.  However,
preferential yielding in the user level schedulers in Power Fortran, Power
C, and the <literal>pthreads</literal> library ensure that critical threads
required for the forward progression of a computation are allocated CPUs (in
preferential yielding, other user threads yield CPUs to the critical threads).
</para>
</section>
<section id="Z899899622janelle">
<title>Synchronized Execution</title>
<para>Synchronized execution is similar to gang scheduling, where related
processes or threads can be run as a group. However with synchronized execution,
all the threads in the group or gang are always scheduled to execute simultaneously
on distinct nodes, using one-to-one mapping. When the time&ndash;slice expires,
all of the threads in the gang are preempted and rescheduled at the same time.
 <comment>(Need more info&mdash;Is this supported yet, or should we point
to Miser for this functionality?)</comment></para>
</section>
<section id="Z899899634janelle">
<title>Coordinating Interactive and Batch Jobs </title>
<para>Larger computers need to support two very different types of use: interactive
and batch applications. These place different, and often conflicting, demands
on the system and require different scheduling disciplines.</para>
<para>Interactive jobs need process resources to be consistently available,
require immediate service to keep response times short, and are more unpredictable
than batch jobs. With batch jobs, many jobs are scheduled simultaneously,
the resource requirements may exceed the capacity of the system, and they
often carry specifications of CPU time and memory. Batch jobs also typically
require a large amount of uninterrupted CPU time and memory and need a fast
turnaround with guarantees about CPU execution time.</para>
<para><comment>(<?Pub Caret>Need more info. on how best to configure for specific
outcomes.)</comment></para>
</section>
<section id="Z899899653janelle">
<title>Scheduling Policies</title>
<para><comment>(Need info on things to consider when scheduling, tips on scheduling,
configuration issues, etc. For example, the issue with not being able to schedule
MPI applications in gang mode and why it is this way; oversubscription issues,
etc.)</comment></para>
</section>
<section id="Z899899667janelle">
<title>Related Commands</title>
<para>The following commands are useful for scheduling purposes: </para>
<para><comment>(Will this list be helpful for administrators? If so, what
other commands should be included?)</comment><deflist termlength="wide" id="Z896449583janelle">
<deflistentry>
<term><command sectionref="1">gr_osview</command></term>
<listitem><para>A graphical display of usage of certain types of system resources
such as CPU usage, memory usage, time waiting for I/O, number of interrupts,
etc.</para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="1">gr_top</command></term>
<listitem><para>Displays a sorted list of processes that are using some portion
of the available CPU cycles on a machine.</para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="1">nice</command></term>
<listitem><para>Executes a command at the specified lower CPU scheduling priority.
</para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="1">npri</command></term>
<listitem><para>Modifies the scheduling priority of a process.</para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="1">osview</command></term>
<listitem><para>Monitors and displays operating system activity. The display
can be dynamically altered to display all or just some of the system counters.
</para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="1">padc</command></term>
<listitem><para>A process activity data collector.</para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="1">par</command></term>
<listitem><para>A process activity reporter that reports on system call and
scheduling activity for one or more processes. <command>par</command> can
be used to trace the activity of a single process, a related group of processes,
or the system as a whole.</para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="1m">passmgmt</command></term>
<listitem><para>Provides password files management functions.</para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="4">proc</command></term>
<listitem><para>A process (debug) filesystem.</para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="1">ps</command></term>
<listitem><para>Reports process status and prints information about active
processes.</para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="1m">pset</command></term>
<listitem><para>Displays and manages processor set information.</para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="5">pthreads</command></term>
<listitem><para>An introduction to POSIX thread characteristics.</para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="5">realtime</command></term>
<listitem><para>An introduction to realtime processing facilities.</para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="1m">renice</command></term>
<listitem><para>Alters the scheduling priority of one or more running processes.
</para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="1">runon</command></term>
<listitem><para>Forces a single threaded process to always be scheduled on
the same processor.</para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="1">sar</command></term>
<listitem><para>System activity reporter for such system counters as CPU utilization,
buffer usage, disk I/O activity, TTY device activity, switching, system call
activity, file access, queue activity, interprocess communications, paging,
and graphics.</para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="2">schedctl</command></term>
<listitem><para>Finds the time slice size.</para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="2">sched_get_priority_max</command></term>
<listitem><para>Determines the maximum allowable priority.</para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="2">sched_get_priority_min</command></term>
<listitem><para>Determines the minimum allowable priority.</para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="2">sched_getparam</command></term>
<listitem><para>Determines the policy or priority of a program.</para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="2">sched_get_priority_max</command></term>
<listitem><para>Gets the scheduling policy priority maximum range.</para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="2">sched_get_priority_min</command></term>
<listitem><para>Gets the scheduling policy priority minimum range.</para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="2">sched_getscheduler</command></term>
<listitem><para>Gets the scheduling policy of a process.</para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="2">sched_rr_get_interval</command></term>
<listitem><para>Finds the time slice size for round-robin scheduling.</para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="2">sched_setparam</command></term>
<listitem><para>Sets the priority from within the program.</para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="2">sched_setscheduler</command></term>
<listitem><para>Sets the policy from within the program.</para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="2">sched_yield</command></term>
<listitem><para>Causes the calling process to relinquish the processor to
a runnable process of higher or equal priority. In the event a higher or equal
priority process is not available, the calling process reacquires control
of the processor.</para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="5">timers</command></term>
<listitem><para>Provides timers and process time accounting information.</para>
</listitem></deflistentry>
<deflistentry>
<term><command sectionref="1">top</command></term>
<listitem><para>Displays the top processes on the system.</para>
</listitem></deflistentry>
</deflist></para>
<para><comment>Others??????</comment> </para>
</section>
</chapter>
<?Pub *0000042148>
