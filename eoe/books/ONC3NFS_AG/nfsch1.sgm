<!-- Produced by version 3.14 (11/22/96) of SGI Frame/SGML translator -->
<CHAPTER LBL="1"><TITLE><XREFTARGET ID="25730">Understanding ONC3/NFS</TITLE><PARAGRAPH>This chapter introduces the SGI implementation of the Sun Microsystems Open Network Computing Plus (ONC+) distributed services, which was previously referred to as Network File System (NFS). In this guide, NFS refers to the distributed network file system in ONC3/NFS. </PARAGRAPH>
<PARAGRAPH>The information in this chapter is prerequisite to successful ONC3/NFS administration. It defines ONC3/NFS and its relationship to other network software, introduces the ONC3/NFS vocabulary, and identifies the software elements that support ONC3/NFS operation. It also explains special utilities and implementation features of ONC3/NFS.</PARAGRAPH>
<PARAGRAPH>This chapter contains these sections:</PARAGRAPH>
<BULLETLIST><BULLET><PARAGRAPH><XREF IDREF="12953" TYPE="TITLE">&ldquo;Overview of ONC3/NFS&rdquo;</XREF></PARAGRAPH>
</BULLET>
<BULLET><PARAGRAPH><XREF IDREF="23035" TYPE="TITLE">&ldquo;Client-Server Fundamentals&rdquo;</XREF></PARAGRAPH>
</BULLET>
<BULLET><PARAGRAPH><XREF IDREF="18818" TYPE="TITLE">&ldquo;NFS Automatic Mounting&rdquo;</XREF></PARAGRAPH>
</BULLET>
<BULLET><PARAGRAPH><XREF IDREF="33258" TYPE="TITLE">&ldquo;UDP Stateless Protocol&rdquo;</XREF></PARAGRAPH>
</BULLET>
<BULLET><PARAGRAPH><XREF IDREF="39875" TYPE="TITLE">&ldquo;NFS Input/Output Management&rdquo;</XREF></PARAGRAPH>
</BULLET>
<BULLET><PARAGRAPH><XREF IDREF="23277" TYPE="TITLE">&ldquo;NFS File Locking Service&rdquo;</XREF></PARAGRAPH>
</BULLET>
</BULLETLIST>
<SECTION1 LBL="" HELPID = ""><TITLE><XREFTARGET ID="12953">Overview of ONC3/NFS </TITLE><PARAGRAPH>ONC3/NFS is the SGI implementation of ONC+ distributed services. ONC3/NFS is optimized for SGI systems, and integrated with the IRIX Interactive Desktop environment and system toolchest. ONC3/NFS can run only on a SGI system.</PARAGRAPH>
<PARAGRAPH>ONC3/NFS is made up of distributed services that allow users to access file systems and directories on remote systems and treat them as if they were local. Networks with heterogeneous architectures and operating systems can participate in the same ONC3/NFS service. The service can also include systems connected to different types of networks.</PARAGRAPH>
<PARAGRAPH>ONC3/NFS is a separate software product, and must be installed on both server and client. However, you should be familiar with the information in this chapter before setting up or modifying the ONC3/NFS environment. </PARAGRAPH>
</SECTION1>
<SECTION1 LBL="" HELPID = ""><TITLE>ONC3/NFS Components</TITLE><PARAGRAPH>This section summarizes the components of ONC3/NFS, and is followed by expanded notes.</PARAGRAPH>
<HANGLIST><HANGPAIR><HANGITEM>NFS</HANGITEM>
<HANGBODY><PARAGRAPH>The Network File System (NFS) is the distributed network file system in ONC3/NFS and contains the automatic mounters and lock manager. ONC3/NFS supports NFS version 3 (NFS3) and NFS version 2, but uses NFS3 by default. NFS is multi-threaded to take advantage of multiprocessor performance. For more about NFS, see <XREF IDREF="40223" TYPE="TEXT">page&nbsp;4</XREF>.</PARAGRAPH>
</HANGBODY>
</HANGPAIR>
<HANGPAIR><HANGITEM>NIS</HANGITEM>
<HANGBODY><PARAGRAPH>The network information service (NIS) is a database of network entity location information that can be used by NFS. NIS is implemented as part of the Unified Name Service (UNS). Information about NIS and UNS is published in a separate volume called the <INDEXTARGET ID="nfsch11"><!-- POSTPROCESSDATA: nfsch11|NIS:documentation --><DOCTITLE>NIS Administration Guide</DOCTITLE>. For more about the interaction of NFS with NIS, see <XREF IDREF="22655" TYPE="TEXT">page&nbsp;5</XREF>.</PARAGRAPH>
</HANGBODY>
</HANGPAIR>
<HANGPAIR><HANGITEM>AutoFS<INDEXTARGET ID="nfsch12"><!-- POSTPROCESSDATA: nfsch12|AutoFS --></HANGITEM>
<HANGBODY><PARAGRAPH>The AutoFS file system (AutoFS), introduced in IRIX 6.2, is an implementation of the automatic mounter that uses the <COMMAND>autofs</COMMAND> command instead of <COMMAND>automount</COMMAND>. Like <COMMAND>automount</COMMAND>, <COMMAND>autofs</COMMAND> provides automatic and transparent NFS mounts upon access of specified AutoFS file systems. <COMMAND>autofs</COMMAND> mainly differs from <COMMAND>automount</COMMAND> by providing multi-threaded service , by providing in-place mounts, and by using the LoFS (loopback file system) to access local file systems. <COMMAND>autofs</COMMAND> is multi-threaded; it accepts dynamic configuration updates. Unlike automount, <COMMAND>autofs</COMMAND> access cannot be blocked by a server that is down or responding slowly. One thread may block, but this does not prevent other references through <COMMAND>autofs</COMMAND> from completing. <BOLD>autofs</BOLD><BOLD>&space;and </BOLD><BOLD>automount</BOLD><BOLD>&space;cannot exist on the same system.</BOLD> By default, <COMMAND>autofs </COMMAND>is enabled upon installation, although <COMMAND>automount</COMMAND> can be selected with <COMMAND>chkconfig</COMMAND>. Further information about AutoFS starts on <XREF IDREF="32620" TYPE="TEXT">page&nbsp;5</XREF>.</PARAGRAPH>
</HANGBODY>
</HANGPAIR>
<HANGPAIR><HANGITEM>CacheFS</HANGITEM>
<HANGBODY><PARAGRAPH>The Cache File System (CacheFS), introduced in IRIX 5.3, provides client-side caching for NFS and other file system types. Using CacheFS on NFS clients with local disk space can significantly increase the number of clients a server can support and reduce the data access time for clients using read-only file systems. For more about CacheFS, refer to <XREF IDREF="11882" TYPE="TEXT">page&nbsp;7</XREF>.</PARAGRAPH>
</HANGBODY>
</HANGPAIR>
<HANGPAIR><HANGITEM>Bulk Data Service</HANGITEM>
<HANGBODY><PARAGRAPH>&lbreak;The SGI implementation of the Bulk Data Service protocol, BDSpro, is available as an option for NFS. BDSpro is an extension to NFS for handling large file transactions over high-speed networks. BDSpro exploits the data access speed of the XFS filesystem and data transfer rates of network media, such as HIPPI and fiberchannel, to accelerate standard NFS performance. The BDS protocol modifies NFS functions to reduce the time needed to transfer files of 100 megabytes or larger over a network connection. For more information about BDSpro, refer to <DOCTITLE>Getting Started With BDSpro</DOCTITLE>.</PARAGRAPH>
</HANGBODY>
</HANGPAIR>
</HANGLIST>
</SECTION1>
<SECTION1 LBL="" HELPID = ""><TITLE><XREFTARGET ID="40223">About NFS</TITLE><PARAGRAPH>NFS is a network service that allows users to access file hierarchies across a network in such a way that they appear to be local. File hierarchies can be entire file systems or individual directories. Systems participating in the NFS service can be heterogeneous. They may be manufactured by different vendors, use different operating systems, and be connected to networks with different architectures. These differences are transparent to the NFS application.<INDEXTARGET ID="nfsch13"><!-- POSTPROCESSDATA: nfsch13|NFS:definition --></PARAGRAPH>
<PARAGRAPH>NFS is an application layer service that can be used on a network running the User Datagram Protocol (UDP) or Transmission Control Protocol (TCP). The UDP protocol has traditionally been used as the transport layer protocol. UDP supports connectionless transmissions and stateless protocol, providing robust service. The TCP protocol supports connection-based transmissions that are beneficial in WAN configurations. TCP provides high reliability, and with its sophisticated packet tracking scheme, reduces client and server input buffer overflow and multiple packet resends. <INDEXTARGET ID="nfsch14"><!-- POSTPROCESSDATA: nfsch14|remote procedure call (RPC):and NFS --></PARAGRAPH>
<PARAGRAPH>NFS relies on<ITALICS>&space;remote procedure call</ITALICS>s (RPC) for session layer services and <ITALICS>external data representation</ITALICS> (XDR) for presentation layer services. XDR is a library of routines that translate data formats between processes. </PARAGRAPH>
<PARAGRAPH><INDEXTARGET ID="nfsch15"><!-- POSTPROCESSDATA: nfsch15|NFS:and OSI model --><XREF IDREF="17312" TYPE="GRAPHIC">Figure&nbsp;1-1</XREF> illustrates the NFS software implementation in the context of the Open Systems Interconnect (OSI) model.</PARAGRAPH>
<!-- RASTERCONVERT: nfsch1.cgm -->
<PARAGRAPH><FIGURE><GRAPHIC FILE="nfsch1-1.gif" POSITION="INLINE" SCALE="FALSE"><CAPTION LBL="1-1"><PREFIX>Figure 1-1 </PREFIX><XREFTARGET ID="17312">NFS Software Implementation</CAPTION>
</FIGURE>
</PARAGRAPH>
<SECTION2 LBL="" HELPID = ""><TITLE><XREFTARGET ID="23985">NFS and Diskless Workstations</TITLE><PARAGRAPH>It is possible to set up a system so that all the required software, including the operating system, is supplied from remote systems by means of the NFS service. Workstations operating in this manner are considered <INDEXTARGET ID="nfsch16"><!-- POSTPROCESSDATA: nfsch16|diskless workstations --><ITALICS>diskless workstations</ITALICS>, even though they may be equipped with a local disk.</PARAGRAPH>
<PARAGRAPH>Instructions for implementing diskless workstations are given in the <DOCTITLE>Diskless Workstation Administration Guide</DOCTITLE>. However, it is important to acquire a working knowledge of NFS before setting up a diskless system.</PARAGRAPH>
</SECTION2>
</SECTION1>
<SECTION1 LBL="" HELPID = ""><TITLE><XREFTARGET ID="22655">About NFS and the Network Information Service</TITLE><PARAGRAPH>The Network Information Service (NIS) is a database service that provides location information about network entities to other network servers and applications, such as NFS. NFS and NIS are independent services that may or may not be operating together on a given network. On networks running NIS, NFS may use the NIS databases to locate systems when NIS queries are specified.<INDEXTARGET ID="nfsch17"><!-- POSTPROCESSDATA: nfsch17|NIS:definition --></PARAGRAPH>
</SECTION1>
<SECTION1 LBL="" HELPID = ""><TITLE>About UNS and NIS</TITLE><PARAGRAPH>The Unified Name Service (UNS) provides a system wide interface to hostname, password and many other lookups. It controls the resolution of hostnames used by AutoFS and automount. Both AutoFS and automount bypass UNS when using information from NIS. <INDEXTARGET ID="nfsch18"><!-- POSTPROCESSDATA: nfsch18|United Name Service(UNS)NIS:and UNS --></PARAGRAPH>
</SECTION1>
<SECTION1 LBL="" HELPID = ""><TITLE><XREFTARGET ID="32620">About The AutoFS File System<INDEXTARGET ID="nfsch19"><!-- POSTPROCESSDATA: nfsch19|AutoFS file system --></TITLE><PARAGRAPH>AutoFS is the kernel virtual file system that supports automatic mounting of file systems. Together with the implementation of <COMMAND>autofsd</COMMAND> (the <COMMAND>autofs</COMMAND> daemon), AutoFS solves several fundamental problems with the earlier implementation of <COMMAND>automount</COMMAND> daemon:</PARAGRAPH>
<BULLETLIST><BULLET><PARAGRAPH>The symbolic links and the <FILENAME>/tmp_mnt</FILENAME> prepended to paths are replaced by in-place mounting.</PARAGRAPH>
</BULLET>
<BULLET><PARAGRAPH>AutoFS is file system independent.</PARAGRAPH>
</BULLET>
</BULLETLIST>
<PARAGRAPH>By default, AutoFS tries NFS version 3 first, and if the server does not support version 3, AutoFS retries the mount using NFS2.</PARAGRAPH>
<PARAGRAPH>Without symbolic links, indirection to mount points is now performed entirely within the kernel, improving performance. <COMMAND>Autofsd</COMMAND> is now a stateless daemon, responsible for performing automatic mounts and unmounts. It allows mount points to be added or deleted without rebooting. The daemon is not required to access a filesystem once it is mounted.</PARAGRAPH>
<PARAGRAPH>In addition, <COMMAND>autofsd</COMMAND> can mount file systems besides NFS such as removable-media file systems. These improvements are compatible with previously existing maps and administrative procedures.</PARAGRAPH>
<SECTION2 LBL="" HELPID = ""><TITLE>Simplified autofs Operation</TITLE><PARAGRAPH>The automount daemon, <COMMAND>autofsd, <INDEXTARGET ID="nfsch110"><!-- POSTPROCESSDATA: nfsch110|<COMMAND>autofsd</COMMAND> daemon --></COMMAND>starts at boot time from the <FILENAME>/etc/init.d/network</FILENAME> script since by default, <COMMAND>autofs</COMMAND> and <COMMAND>nfs</COMMAND> are <COMMAND>chkconfig</COMMAND>'d on. The <FILENAME>/etc/init.d/network</FILENAME> script also runs the <COMMAND>autofs</COMMAND> command, which reads a master map and installs AutoFS mount points.</PARAGRAPH>
<PARAGRAPH>Unlike <COMMAND>mount</COMMAND>, <COMMAND>autofs</COMMAND> does not read the file <COMMAND>/etc/fstab</COMMAND>, which is specific to each workstation, for a list of file systems to mount. Rather, <COMMAND>autofs</COMMAND> is controlled within a domain (and on particular workstations) through the maps, saving a great deal of administrator time.</PARAGRAPH>
<SECTION3 LBL="" HELPID = ""><TITLE>How Autofs Navigates Through the Network (Maps) </TITLE><PARAGRAPH><COMMAND>Autofs</COMMAND> searches a series of maps to navigate its way through the network. Maps are files that contain information mapping local directories or mount points to remote server file systems. A special map, <CMDLINEOPT>-hosts</CMDLINEOPT>, is supported by AutoFS to provide a convenient way of accessing all host machines on the network. Maps are available locally or through a network name service like NIS or NIS+. You create maps to meet the needs of your users' environment. See <XREF IDREF="18818" TYPE="TITLE">&ldquo;NFS Automatic Mounting&rdquo;</XREF>, and <XREF IDREF="23604" TYPE="TITLE">Chapter&nbsp;3, &ldquo;Using Automatic Mounter Map Options&rdquo;</XREF> for detailed information on automatic mounting and its maps.</PARAGRAPH>
</SECTION3>
</SECTION2>
</SECTION1>
<SECTION1 LBL="" HELPID = ""><TITLE><XREFTARGET ID="11882">About CacheFS File System</TITLE><PARAGRAPH>A <INDEXTARGET ID="nfsch111"><!-- POSTPROCESSDATA: nfsch111|cached file systems: definition --><VARIABLE>cache</VARIABLE> is a temporary storage area for data. With the cache file system (CacheFS), you can store frequently used data from a remote file system or CD-ROM on the local disk drive of a workstation. The data stored on the local disk is the cache.</PARAGRAPH>
<PARAGRAPH>When a file system is cached, the data is read from the original file system and stored on the local disk. The reduction in network traffic improves performance. If the remote file system is on a storage medium with slower response time than the local disk (such as a CD&ndash;ROM), caching provides an additional performance gain.</PARAGRAPH>
<PARAGRAPH>CacheFS can use all or part of a local disk to store data from one or more remote file systems. A user accessing a file does not need to know whether the file is stored in a cache or is being read from the original file system. The user opens, reads, and writes files as usual. </PARAGRAPH>
<PARAGRAPH>A cache with default parameters can be created with the <COMMAND>mount</COMMAND> command. Default parameters can be changed with the <COMMAND>cfsadmin</COMMAND> command. See <XREF IDREF="32549" TYPE="TITLE">&ldquo;Cached File System Administration&rdquo;</XREF> and <XREF IDREF="59635" TYPE="TITLE">&ldquo;Cache Resource Parameters in CacheFS&rdquo;</XREF>. Specific details of CacheFS are discussed in <XREF IDREF="33373" TYPE="TITLE">&ldquo;Planning a CacheFS File System&rdquo; in Chapter&nbsp;2</XREF>.</PARAGRAPH>
</SECTION1>
<SECTION1 LBL="" HELPID = ""><TITLE><XREFTARGET ID="23035">Client-Server Fundamentals</TITLE><PARAGRAPH>In an NFS transaction, the workstation requesting access to remote directories is known as the <INDEXTARGET ID="nfsch112"><!-- POSTPROCESSDATA: nfsch112|client-server model --><INDEXTARGET ID="nfsch113"><!-- POSTPROCESSDATA: nfsch113|client:definition --><INDEXTARGET ID="nfsch114"><!-- POSTPROCESSDATA: nfsch114|server:definition --><ITALICS>client</ITALICS>. The workstation providing access to its local directories is known as the <ITALICS>server</ITALICS>. A workstation can function as a client and a server simultaneously. It can allow remote access to its local file systems while accessing remote directories with NFS. The client-server relationship is established by two complementary processes, <ITALICS>exporting</ITALICS> and <ITALICS>mounting</ITALICS>.</PARAGRAPH>
<SECTION2 LBL="" HELPID = ""><TITLE>Exporting NFS File Systems</TITLE><PARAGRAPH><INDEXTARGET ID="nfsch115"><!-- POSTPROCESSDATA: nfsch115|exporting:definition --><ITALICS>Exporting</ITALICS> is the process by which an NFS server provides access to its file resources to remote clients. Individual directories, as well as file systems, can be exported, but exported entities are usually referred to as file systems. Exporting is done either during the server's boot sequence or from a command line as superuser while the server is running. </PARAGRAPH>
<PARAGRAPH>Once a file system is exported, any authorized client can use it. A list of exported file systems, client authorizations, and other export options are specified in the <INDEXTARGET ID="nfsch116"><!-- POSTPROCESSDATA: nfsch116|unexporting, definition --><FILENAME>/etc/exports</FILENAME> file (see <XREF IDREF="26813" TYPE="TITLE">&ldquo;Operation of /etc/exports and Other Export Files&rdquo; in Chapter&nbsp;2</XREF> for details). Exported file systems are removed from NFS service by a process known as <ITALICS>unexporting</ITALICS>. </PARAGRAPH>
<PARAGRAPH>A server can export any file system or directory that is local. However, it cannot export both a parent and child directory within the same file system; to do so is redundant.<INDEXTARGET ID="nfsch117"><!-- POSTPROCESSDATA: nfsch117|exporting:restrictions --></PARAGRAPH>
<PARAGRAPH>For example, assume that the file system <INDEXTARGET ID="nfsch118"><!-- POSTPROCESSDATA: nfsch118|exporting:parent and child directories --><FILENAME>/usr</FILENAME> contains the directory <FILENAME>/usr/demos</FILENAME>. As the child of <FILENAME>/usr</FILENAME><ITALICS>,</ITALICS>&space;<FILENAME>/usr/demos</FILENAME> is automatically exported with <FILENAME>/usr</FILENAME>. For this reason, attempting to export both <FILENAME>/usr</FILENAME> and <FILENAME>/usr/demos</FILENAME> generates an error message that the parent directory is already exported. If <FILENAME>/usr</FILENAME> and <FILENAME>/usr/demos</FILENAME> were separate file systems, this example would be valid.</PARAGRAPH>
<PARAGRAPH>When exporting hierarchically related file systems such as <FILENAME>/usr</FILENAME> and <FILENAME>/usr/demos</FILENAME> in the previous example, we recommend the use of the <CMDLINEOPT>-nohide</CMDLINEOPT> option to reduce the number of mounts required by clients (see<REFPAGE>&space;exports(4)</REFPAGE>).</PARAGRAPH>
</SECTION2>
<SECTION2 LBL="" HELPID = ""><TITLE>Mounting NFS File Systems</TITLE><PARAGRAPH><INDEXTARGET ID="nfsch119"><!-- POSTPROCESSDATA: nfsch119|mounting:definition --><ITALICS>Mounting</ITALICS> is the process by which file systems, including NFS file systems, are made available to the IRIX operating system and consequently, the user. When NFS file systems or directories are mounted, they are made available to the client over the network by a series of remote procedure calls that enable the client to access the file system transparently from the server's disk. Mounted NFS directories or file systems are not physically present on the client system, but the mount looks like a local mount and users enter commands as if the file systems were local.</PARAGRAPH>
<PARAGRAPH>NFS clients can have directories mounted from several servers simultaneously. Mounting can be done as part of the client's boot sequence, automatically, at file system access, with the help of a user-level daemon, or with a superuser command after the client is running. When mounted directories are no longer needed, they can be relinquished in a process known as <INDEXTARGET ID="nfsch120"><!-- POSTPROCESSDATA: nfsch120|unmounting:definition --><ITALICS>unmounting</ITALICS>.</PARAGRAPH>
<PARAGRAPH>Like locally mounted file systems, NFS mounted file systems and directories can be specified in the <FILENAME>/etc/fstab</FILENAME> file (see <XREF IDREF="56149" TYPE="TITLE">&ldquo;Operation of /etc/fstab and Other Mount Files&rdquo; in Chapter&nbsp;2</XREF> for details). Since NFS file systems are located on remote systems, specifications for NFS mounted resources must include the name of the system where they reside.</PARAGRAPH>
<SECTION3 LBL="" HELPID = ""><TITLE>NFS Mount Points</TITLE><PARAGRAPH>The access point in the client file system where an NFS directory is attached is known as a <INDEXTARGET ID="nfsch121"><!-- POSTPROCESSDATA: nfsch121|mount points:definition --><ITALICS>mount point</ITALICS>. A mount point is specified by a conventional IRIX pathname.</PARAGRAPH>
<PARAGRAPH><INDEXTARGET ID="nfsch122"><!-- POSTPROCESSDATA: nfsch122|mounting:illustration --><XREF IDREF="32513" TYPE="GRAPHIC">Figure&nbsp;1-2</XREF> illustrates the effect of mounting directories onto mount points on an NFS client.</PARAGRAPH>
<!-- RASTERCONVERT: nfsch1.cgm2 -->
<PARAGRAPH><FIGURE><GRAPHIC FILE="nfsch1-2.gif" POSITION="INLINE" SCALE="FALSE"><CAPTION LBL="1-2"><PREFIX>Figure 1-2 </PREFIX><XREFTARGET ID="32513">Sample Mounted Directory</CAPTION>
</FIGURE>
</PARAGRAPH>
<PARAGRAPH>The pathname of a file system on a server can be different from its mount point on the client. For example, in <INDEXTARGET ID="nfsch123"><!-- POSTPROCESSDATA: nfsch123|exported filesystems:different pathname --><XREF IDREF="32513" TYPE="GRAPHIC">Figure&nbsp;1-2</XREF> the file system <FILENAME>/usr/demos</FILENAME> is mounted in the client's file system at mount point <FILENAME>/n/demos</FILENAME>. Users on the client gain access to the mounted directory with a conventional <COMMAND>cd</COMMAND> command to <FILENAME>/n/demos</FILENAME>, as if the directory were local.</PARAGRAPH>
</SECTION3>
<SECTION3 LBL="" HELPID = ""><TITLE><XREFTARGET ID="57726">NFS Mount Restrictions</TITLE><PARAGRAPH>NFS does not permit <INDEXTARGET ID="nfsch124"><!-- POSTPROCESSDATA: nfsch124|multihopping --><INDEXTARGET ID="nfsch125"><!-- POSTPROCESSDATA: nfsch125|mounting:restrictions --><ITALICS>multihopping</ITALICS>, mounting a directory that is itself NFS mounted on the server. For example, if <ITALICS>host1</ITALICS> mounts <FILENAME>/usr/demos</FILENAME> from <ITALICS>host2</ITALICS>, <ITALICS>host3</ITALICS> cannot mount <FILENAME>/usr/demos</FILENAME> from <ITALICS>host1</ITALICS>. This would constitute a <ITALICS>multihop</ITALICS>.</PARAGRAPH>
<PARAGRAPH>NFS also does not permit <INDEXTARGET ID="nfsch126"><!-- POSTPROCESSDATA: nfsch126|loopback mounting, definition --><ITALICS>loopback mounting</ITALICS>, mounting a directory that is local to the client via NFS. For example, the local file system <FILENAME>/usr</FILENAME> on <ITALICS>host1</ITALICS> cannot be NFS mounted to <ITALICS>host1</ITALICS>, this would constitute a loopback mount.</PARAGRAPH>
</SECTION3>
</SECTION2>
</SECTION1>
<SECTION1 LBL="" HELPID = ""><TITLE><XREFTARGET ID="18818">NFS Automatic Mounting</TITLE><PARAGRAPH>As an alternative to standard mounting via <INDEXTARGET ID="nfsch127"><!-- POSTPROCESSDATA: nfsch127|automatic mounters:definition --><FILENAME>/etc/fstab</FILENAME> or the <COMMAND>mount</COMMAND> command, NFS provides two automatic mounting utilities. The original automatic mounter, called <ITALICS>automount </ITALICS>and a newer implementation introduced in IRIX 6.2, called <ITALICS>autofs</ITALICS>. Both automatic mounters dynamically mount file systems when they are referenced by any user on the client system, then unmount them after a specified time interval. Unlike standard mounting, <COMMAND>automount </COMMAND>and <COMMAND>autofs</COMMAND>, once set up, do not require superuser privileges to mount a remote directory. They also create the mount points needed to access the mounted resource. NFS servers cannot distinguish between directories mounted by the automatic mounters and those mounted by conventional mount procedures. <COMMAND>autofs</COMMAND> and <COMMAND>automount</COMMAND> cannot co-exist on the same system.</PARAGRAPH>
<PARAGRAPH>Unlike the standard mount process<ITALICS>, </ITALICS><COMMAND>automount</COMMAND> and <COMMAND>autofs</COMMAND> do not read the <FILENAME>/etc/fstab</FILENAME> file for mount specifications. Instead, they read alternative files (either local or through NIS) known as maps for mounting information (see <XREF IDREF="22306" TYPE="TITLE">&ldquo;Operation of Automatic Mounter Files and Maps&rdquo;</XREF> for details). They also provide special maps for accessing remote systems and automatically reflecting changes in the <FILENAME>/etc/hosts</FILENAME> file and any changes to the remote server's <FILENAME>/etc/exports</FILENAME> file.</PARAGRAPH>
<PARAGRAPH>Default configuration information for automatic mounting is contained in the files <FILENAME>/etc/config/automount.options</FILENAME> (for <FILENAME>automount</FILENAME>) and<FILENAME>&space;/etc/config/autofs.options</FILENAME> (for <FILENAME>autofs</FILENAME>). These files can be modified to use different options and more sophisticated maps.</PARAGRAPH>
</SECTION1>
<SECTION1 LBL="" HELPID = ""><TITLE><XREFTARGET ID="33258">UDP Stateless Protocol</TITLE><PARAGRAPH>When NFS is used with UDP as its transport protocol, it uses a <INDEXTARGET ID="nfsch128"><!-- POSTPROCESSDATA: nfsch128|stateless protocol --><INDEXTARGET ID="nfsch129"><!-- POSTPROCESSDATA: nfsch129|failure:of server --><INDEXTARGET ID="nfsch130"><!-- POSTPROCESSDATA: nfsch130|failure:of client --><ITALICS>stateless protocol</ITALICS> in which the server maintains almost no information on NFS processes. This stateless protocol insulates clients and servers from the effects of failures. If a server fails, the only effect to clients is that NFS data on the server is unavailable to clients. If a client fails, server performance is not affected.</PARAGRAPH>
<PARAGRAPH>Clients are independently responsible for completing NFS transactions if the server or network fails. By default, when a failure occurs, NFS clients continue attempting to complete the NFS operation until the server or network recovers. To the client, the failure can appear as slow performance on the part of the server. Client applications continue retransmitting until service is restored and their NFS operations can be completed. If a client fails, no action is needed by the server or its administrator in order for the server to continue operation.<INDEXTARGET ID="nfsch131"><!-- POSTPROCESSDATA: nfsch131|failure:of network --></PARAGRAPH>
</SECTION1>
<SECTION1 LBL="" HELPID = ""><TITLE>TCP Connections for NFS</TITLE><PARAGRAPH>The TCP protocol transport option for NFS provides a highly efficient method for transmitting packets, especially in large WAN networks. With the TCP protocol, a connection is made between the client and the server, and all packets are labeled and tracked. Even though this tracking is more CPU-intensive, input buffer overflow and multiple packet resends due to lost packets and timeouts are handled much more efficiently than with UDP. </PARAGRAPH>
</SECTION1>
<SECTION1 LBL="" HELPID = ""><TITLE><XREFTARGET ID="39875">NFS Input/Output Management</TITLE><PARAGRAPH>In NFS2 transactions, data input and output is asynchronous read-ahead and write-behind, unless otherwise specified. As the server receives data, it notifies the client that the data was successfully written. The client responds by freeing the blocks of NFS data successfully transmitted to the server. In reality, however, the server might not write the data to disk before notifying the client, a technique called <INDEXTARGET ID="nfsch132"><!-- POSTPROCESSDATA: nfsch132|delayed writes --><INDEXTARGET ID="nfsch133"><!-- POSTPROCESSDATA: nfsch133|input/output management --><INDEXTARGET ID="nfsch134"><!-- POSTPROCESSDATA: nfsch134|asynchronous data transfer --><ITALICS>delayed writes</ITALICS>. Writes are done when they are convenient for the server, but at least every 30 seconds. NFS2 uses delayed writes by default.</PARAGRAPH>
<PARAGRAPH>With synchronous writes, the server writes the data to disk before notifying the client that it has been written. Synchronous writes are supported as an option in NFS2 (see <INDEXTARGET ID="nfsch135"><!-- POSTPROCESSDATA: nfsch135|synchronous writes --><XREF IDREF="24611" TYPE="TITLE">&ldquo;/etc/exports Options&rdquo; in Chapter&nbsp;2</XREF> for details of NFS options), and in NFS3. Synchronous writes may slow NFS performance due to the time required for disk access, but increase data integrity in the event of system or network failure.</PARAGRAPH>
</SECTION1>
<SECTION1 LBL="" HELPID = ""><TITLE><XREFTARGET ID="23277">NFS File Locking Service</TITLE><PARAGRAPH>To help manage file access conflicts and protect NFS sessions during failures, NFS offers a file and record locking service called the <INDEXTARGET ID="nfsch136"><!-- POSTPROCESSDATA: nfsch136|file locking service. <ITALICS>See</ITALICS> lock manager. --><INDEXTARGET ID="nfsch137"><!-- POSTPROCESSDATA: nfsch137|network lock manager. <ITALICS>See</ITALICS> lock manager. --><INDEXTARGET ID="nfsch138"><!-- POSTPROCESSDATA: nfsch138|lock manager:application calls --><INDEXTARGET ID="nfsch139"><!-- POSTPROCESSDATA: nfsch139|lock manager:description --><INDEXTARGET ID="nfsch140"><!-- POSTPROCESSDATA: nfsch140|remote procedure call (RPC):and lock manager --><ITALICS>network lock manager</ITALICS>. The network lock manager is a separate service NFS makes available to user applications. To use the locking service, applications must make calls to standard IRIX lock routines (see the reference pages<REFPAGE>&space;fcntl(2)</REFPAGE>,<REFPAGE>&space;flock(3B)</REFPAGE>, and<REFPAGE>&space;lockf(3C)</REFPAGE>). For NFS files, these calls are sent to the network lock manager process (see<REFPAGE>&space;lockd(1M)</REFPAGE>) on the server. </PARAGRAPH>
<PARAGRAPH>The network lock manager processes must run on both client and server. Communication between the two processes is by means of RPC. Calls issued to the client process are handed to the server process, which uses its local IRIX locking utilities to handle the call. If the file is in use, the lock manager issues an advisory to the calling application, but it does not prevent the application from accessing a busy file. The application must determine how to respond to the advisory, using its own facilities.</PARAGRAPH>
<PARAGRAPH>Despite the fact that the network lock manager adheres to<COMMAND>&space;lockf </COMMAND>and<COMMAND>&space;fcntl</COMMAND> semantics, its operating characteristics are influenced by the nature of the network, particularly during crashes. </PARAGRAPH>
<SECTION2 LBL="" HELPID = ""><TITLE>NFS Locking and Crash Recovery</TITLE><PARAGRAPH>As part of the file locking service, the network lock manager assists with crash recovery by maintaining state information on locked files. It uses this information to reconstruct locks in the event of a server or client failure. <INDEXTARGET ID="nfsch141"><!-- POSTPROCESSDATA: nfsch141|crash recovery:and lock manager --><INDEXTARGET ID="nfsch142"><!-- POSTPROCESSDATA: nfsch142|failure:of network --><INDEXTARGET ID="nfsch143"><!-- POSTPROCESSDATA: nfsch143|lock manager:crash recovery --><INDEXTARGET ID="nfsch144"><!-- POSTPROCESSDATA: nfsch144|failure:of client --><INDEXTARGET ID="nfsch145"><!-- POSTPROCESSDATA: nfsch145|failure:of server --></PARAGRAPH>
<PARAGRAPH>When an NFS client goes down, the lock managers on all of its servers are notified by their status monitors, and they simply release their locks, on the assumption that the client will request them again when it wants them. When a server crashes, however, matters are different. When the server comes back up, its lock manager gives the client lock managers a grace period to submit lock reclaim requests. During this period, the lock manager accepts only reclaim requests. The client status monitors notify their respective lock managers when the server recovers. The default grace period is 45 seconds.</PARAGRAPH>
<PARAGRAPH>After a server crash, a client may not be able to recover a lock that it had on a file on that server, because another process may have beaten the recovering application process to the lock. In this case the SIGLOST signal is sent to the process (the default action for this signal is to kill the application).</PARAGRAPH>
</SECTION2>
<SECTION2 LBL="" HELPID = ""><TITLE>NFS Locking and the Network Status Monitor</TITLE><PARAGRAPH>To handle crash recoveries, the network lock manager relies on information provided by the <INDEXTARGET ID="nfsch146"><!-- POSTPROCESSDATA: nfsch146|network status monitor --><INDEXTARGET ID="nfsch147"><!-- POSTPROCESSDATA: nfsch147|crash recovery:and network status monitor --><ITALICS>network status monitor</ITALICS>. The network status monitor is a general service that provides information about network systems to network services and applications. The network status monitor notifies the network lock manager when a network system recovers from a failure, and by implication, that the system failed. This notification alerts the network lock manager to retransmit lock recovery information to the server.</PARAGRAPH>
<PARAGRAPH>To use the network status monitor, the network lock manager registers with the status monitor process (see<INDEXTARGET ID="nfsch148"><!-- POSTPROCESSDATA: nfsch148|<ITALICS>statd</ITALICS> daemon --><REFPAGE>&space;statd(1M)</REFPAGE>) the names of clients and servers for which it needs information. The network status monitor then tracks the status of those systems and notifies the network lock manager when one of them recovers from a failure.</PARAGRAPH>
</SECTION2>
</SECTION1>
</CHAPTER>
