<!-- Produced by version 3.14 (11/22/96) of SGI Frame/SGML translator -->
<CHAPTER LBL="6"><TITLE><XREFTARGET ID="93712">Managing Device Interactions</TITLE><PARAGRAPH>A real-time program is defined by its close relationship to external hardware. This chapter reviews the ways that IRIX gives you to access and control external devices.</PARAGRAPH>
<SECTION1 LBL="" HELPID = ""><TITLE><XREFTARGET ID="85724">Device Drivers</TITLE><NOTE><PREFIX>Note</PREFIX>This section contains an overview for readers who are not familiar with the details of the UNIX I/O system. All these points are covered in much greater detail in the <DOCTITLE>IRIX Device Driver Programmer's Guide</DOCTITLE> (see <XREF IDREF="96700" TYPE="TITLE">&ldquo;Other Useful Books&rdquo;</XREF>).</NOTE>
<PARAGRAPH>It is a basic concept in UNIX that all I/O is done by reading or writing files. All I/O devices&mdash;disks, tapes, printers, terminals, and VME cards&mdash;are represented as files in the file system. Conventionally, every physical device is represented by an entry in the <FILENAME>/dev</FILENAME> file system hierarchy. The purpose of each <GLOSSARYITEM>device special file</GLOSSARYITEM> is to associate a device name with a a <GLOSSARYITEM>device driver</GLOSSARYITEM>, a module of code that is loaded into the kernel either at boot time or dynamically, and is responsible for operating that device at the kernel's request.</PARAGRAPH>
<SECTION2 LBL="" HELPID = ""><TITLE>How Devices Are Defined</TITLE><PARAGRAPH>In IRIX 6.4 and later, the <FILENAME>/dev</FILENAME> filesystem still exists to support programs and shell scripts that depend on conventional names such as <FILENAME>/dev/tty</FILENAME>. However, the true representation of all devices is built in a different file system rooted at <FILENAME>/hw</FILENAME> (for hardware). You can explore the <FILENAME>/hw</FILENAME> filesystem using standard commands such as <COMMAND>file</COMMAND>, <COMMAND>ls</COMMAND>, and <COMMAND>cd</COMMAND>. You will find that the conventional names in <FILENAME>/dev</FILENAME> are implemented as links to device special files in <FILENAME>/hw</FILENAME>. The creation and use of <FILENAME>/hw</FILENAME>, and the definition of devices in it, is described in detail in the <DOCTITLE>IRIX Device Driver Programmer's Guide</DOCTITLE>. </PARAGRAPH>
</SECTION2>
<SECTION2 LBL="" HELPID = ""><TITLE>How Devices Are Used</TITLE><PARAGRAPH>To use a device, a process opens the device special file by passing the file pathname to <INDEXTARGET ID="RPPG_ch061"><!-- POSTPROCESSDATA: RPPG_ch061|device:opening --><INDEXTARGET ID="RPPG_ch062"><!-- POSTPROCESSDATA: RPPG_ch062|<FUNCTION>open()</FUNCTION>:of a device --><FUNCTION>open()</FUNCTION> (see the <REFPAGE>open(2)</REFPAGE> reference page). For example, a generic SCSI device might be opened by a statement such as this.</PARAGRAPH>
<EXAMPLE>
int scsi_fd = open("/dev/scsi/sc0d11l0",O_RDWR);
</EXAMPLE>
<PARAGRAPH>The returned integer is the <INDEXTARGET ID="RPPG_ch063"><!-- POSTPROCESSDATA: RPPG_ch063|file descriptor:of a device --><GLOSSARYITEM>file descriptor</GLOSSARYITEM>, a number that indexes an array of control blocks maintained by IRIX in the address space of each process. With a file descriptor, the process can call other system functions that give access to the device. Each of these system calls is implemented in the kernel by transferring control to an entry point in the device driver.</PARAGRAPH>
<SECTION3 LBL="" HELPID = ""><TITLE>Device Driver Entry Points</TITLE><PARAGRAPH>Each device driver supports one or more of the following operations:<INDEXTARGET ID="RPPG_ch064"><!-- POSTPROCESSDATA: RPPG_ch064|device driver:entry points to --></PARAGRAPH>
<HANGLIST><HANGPAIR><HANGITEM>open</HANGITEM>
<HANGBODY><PARAGRAPH>Notifies the driver that a process wants to use the device.</PARAGRAPH>
</HANGBODY>
</HANGPAIR>
<HANGPAIR><HANGITEM>close</HANGITEM>
<HANGBODY><PARAGRAPH>Notifies the driver that a process is finished with the device.</PARAGRAPH>
</HANGBODY>
</HANGPAIR>
<HANGPAIR><HANGITEM>interrupt</HANGITEM>
<HANGBODY><PARAGRAPH>Entered by the kernel upon a hardware interrupt, notes an event reported by a device, such as the completion of a device action, and possibly initiates another action.</PARAGRAPH>
</HANGBODY>
</HANGPAIR>
<HANGPAIR><HANGITEM>read</HANGITEM>
<HANGBODY><PARAGRAPH>Entered from the function <INDEXTARGET ID="RPPG_ch065"><!-- POSTPROCESSDATA: RPPG_ch065|<FUNCTION>read()</FUNCTION>:and device driver --><FUNCTION>read()</FUNCTION>, transfers data from the device to a buffer in the address space of the calling process.</PARAGRAPH>
</HANGBODY>
</HANGPAIR>
<HANGPAIR><HANGITEM>write</HANGITEM>
<HANGBODY><PARAGRAPH>Entered from the function <INDEXTARGET ID="RPPG_ch066"><!-- POSTPROCESSDATA: RPPG_ch066|<FUNCTION>write()</FUNCTION>:and device driver --><FUNCTION>write()</FUNCTION>, transfers data from the calling process's address space to the device.</PARAGRAPH>
</HANGBODY>
</HANGPAIR>
<HANGPAIR><HANGITEM>control</HANGITEM>
<HANGBODY><PARAGRAPH>Entered from the function <INDEXTARGET ID="RPPG_ch067"><!-- POSTPROCESSDATA: RPPG_ch067|<FUNCTION>ioctl()</FUNCTION>:and device driver --><FUNCTION>ioctl()</FUNCTION>, performs some kind of control function specific to the type of device in use.	</PARAGRAPH>
</HANGBODY>
</HANGPAIR>
</HANGLIST>
<PARAGRAPH>Not every driver supports every entry point. For example, the generic SCSI driver (see <XREF IDREF="10662" TYPE="TITLE">&ldquo;Generic SCSI Device Driver&rdquo;</XREF>) supports only the open, close, and control entries.</PARAGRAPH>
<PARAGRAPH>Device drivers in general are documented with the device special files they support, in volume 7 of the reference pages. For a sample, review:<INDEXTARGET ID="RPPG_ch068"><!-- POSTPROCESSDATA: RPPG_ch068|device driver:reference pages --></PARAGRAPH>
<BULLETLIST><BULLET><PARAGRAPH><REFPAGE>dsk(7m)</REFPAGE>, documenting the standard IRIX SCSI disk device driver</PARAGRAPH>
</BULLET>
<BULLET><PARAGRAPH><REFPAGE>smfd(7m)</REFPAGE>, documenting the diskette and optical diskette driver</PARAGRAPH>
</BULLET>
<BULLET><PARAGRAPH><REFPAGE>tps(7m)</REFPAGE>, documenting the SCSI tape drive device driver</PARAGRAPH>
</BULLET>
<BULLET><PARAGRAPH><REFPAGE>plp(7)</REFPAGE>, documenting the parallel line printer device driver</PARAGRAPH>
</BULLET>
<BULLET><PARAGRAPH><REFPAGE>klog(7)</REFPAGE>, documenting a &ldquo;device&rdquo; driver that is not a device at all, but a special interface to the kernel</PARAGRAPH>
</BULLET>
</BULLETLIST>
<PARAGRAPH>If you review a sample of entries in volume 7, as well as other reference pages that are called out in the topics in this chapter, you will understand the wide variety of functions performed by device drivers.</PARAGRAPH>
</SECTION3>
</SECTION2>
<SECTION2 LBL="" HELPID = ""><TITLE>Taking Control of Devices</TITLE><PARAGRAPH>When your program needs direct control of a device, you have the following choices:</PARAGRAPH>
<BULLETLIST><BULLET><PARAGRAPH>If it is a device for which IRIX or the device manufacturer distributes a device driver, find the device driver reference page in volume 7 to learn the device driver's support for <FUNCTION>read()</FUNCTION>, <FUNCTION>write()</FUNCTION>, <FUNCTION>mmap()</FUNCTION>, and <FUNCTION>ioctl()</FUNCTION>. Use these functions to control the device.</PARAGRAPH>
</BULLET>
<BULLET><PARAGRAPH>If it is a PCI device without Bus Master capability, you can control it directly from your program using programmed I/O (see the <REFPAGE>pciba(7M)</REFPAGE> reference page). This option is discussed in the <DOCTITLE>IRIX Device Driver Programmer's Guide</DOCTITLE>.</PARAGRAPH>
</BULLET>
<BULLET><PARAGRAPH>If it is a VME device without Bus Master capability, you can control it directly from your program using programmed I/O or user-initiated DMA. Both options are discussed under <XREF IDREF="25650" TYPE="TITLE">&ldquo;The VME Bus&rdquo;</XREF>.</PARAGRAPH>
</BULLET>
<BULLET><PARAGRAPH>If it is a PCI or VME device with Bus Master (on-board DMA) capability, you should receive an IRIX device driver from the OEM. Consult <DOCTITLE>IRIX Admin: System Configuration and Operation</DOCTITLE> to install the device and its driver. Read the OEM reference page to learn the device driver's support for <FUNCTION>read()</FUNCTION>, <FUNCTION>write()</FUNCTION>, and <FUNCTION>ioctl()</FUNCTION>.</PARAGRAPH>
</BULLET>
<BULLET><PARAGRAPH>If it is a SCSI device that does not have built-in IRIX support, you can control it from your own program using the generic SCSI device driver. See <XREF IDREF="10662" TYPE="TITLE">&ldquo;Generic SCSI Device Driver&rdquo;</XREF>.</PARAGRAPH>
</BULLET>
</BULLETLIST>
<PARAGRAPH>In the remaining case, you have a device with no driver. In this case you must create a device driver. This process is documented in the <DOCTITLE>IRIX Device Driver Programmer's Guide</DOCTITLE>, which contains extensive information and sample code (see <XREF IDREF="96700" TYPE="TITLE">&ldquo;Other Useful Books&rdquo;</XREF>).</PARAGRAPH>
</SECTION2>
</SECTION1>
<SECTION1 LBL="" HELPID = ""><TITLE><XREFTARGET ID="94299">SCSI Devices</TITLE><PARAGRAPH>The SCSI interface is the principal way of attaching disk, cartridge tape, CD-ROM, and digital audio tape (DAT) devices to the system. It can be used for other kinds of devices, such as scanners and printers.<INDEXTARGET ID="RPPG_ch069"><!-- POSTPROCESSDATA: RPPG_ch069|SCSI interface --></PARAGRAPH>
<PARAGRAPH>IRIX contains device drivers for supported disk and tape devices. Other SCSI devices are controlled through a generic device driver that must be extended with programming for a specific device.</PARAGRAPH>
<SECTION2 LBL="" HELPID = ""><TITLE>SCSI Adapter Support</TITLE><PARAGRAPH>The detailed, board-level programming of the host SCSI adapters is done by an IRIX-supplied host adapter driver. The services of this driver are available to the SCSI device drivers that manage the logical devices. If you write a SCSI driver, it controls the device indirectly, by calling a host adapter driver.</PARAGRAPH>
<PARAGRAPH>The host adapter drivers handle the low-level communication over the SCSI interface, such as programming the SCSI interface chip or board, negotiating synchronous or wide mode, and handling disconnect/reconnect. SCSI device drivers call on host adapter drivers using indirect calls through a table of adapter functions. The use of host adapter drivers is documented in the <DOCTITLE>IRIX Device Driver Programmer's Guide</DOCTITLE>.</PARAGRAPH>
</SECTION2>
<SECTION2 LBL="" HELPID = ""><TITLE>System Disk Device Driver</TITLE><PARAGRAPH>The naming conventions for disk and tape device files are documented in the <REFPAGE>intro(7)</REFPAGE> reference page. In general, devices in <FILENAME>/dev/</FILENAME>[<FILENAME>r</FILENAME>]<FILENAME>dsk</FILENAME> are disk drives, and devices in <FILENAME>/dev/</FILENAME>[<FILENAME>r</FILENAME>]<FILENAME>mt</FILENAME> are tape drives.</PARAGRAPH>
<PARAGRAPH>Disk devices in <FILENAME>/dev/</FILENAME>[<FILENAME>r</FILENAME>]<FILENAME>dsk</FILENAME> are operated by the SCSI disk controller, which is documented in the <REFPAGE>dsk(7)</REFPAGE> reference page. It is possible for a program to open a disk device and read, write, or memory-map it, but this is almost never done. Instead, programs open, read, write, or map files; and the EFS or XFS file system interacts with the device driver.</PARAGRAPH>
</SECTION2>
<SECTION2 LBL="" HELPID = ""><TITLE>System Tape Device Driver</TITLE><PARAGRAPH>Tape devices in <INDEXTARGET ID="RPPG_ch0610"><!-- POSTPROCESSDATA: RPPG_ch0610|device driver:tape --><INDEXTARGET ID="RPPG_ch0611"><!-- POSTPROCESSDATA: RPPG_ch0611|tape device --><FILENAME>/dev/</FILENAME>[<FILENAME>r</FILENAME>]<FILENAME>mt</FILENAME> are operated by the magnetic tape device driver, which is documented in the <REFPAGE>tps(7)</REFPAGE> reference page. Users normally control tapes using such commands as <COMMAND>tar</COMMAND>, <COMMAND>dd</COMMAND>, and <COMMAND>mt</COMMAND> (see the <REFPAGE>tar(1)</REFPAGE>, <REFPAGE>dd(1M)</REFPAGE> and <REFPAGE>mt(1)</REFPAGE> reference pages), but it is also common for programs to open a tape device and then use <FUNCTION>read()</FUNCTION>, <FUNCTION>write()</FUNCTION>, and <FUNCTION>ioctl()</FUNCTION> to interact with the device driver.</PARAGRAPH>
<PARAGRAPH>Since the tape device driver supports the read/write interface, you can schedule tape I/O through the asynchronous I/O interface (see <XREF IDREF="73732" TYPE="TITLE">&ldquo;Asynchronous I/O Basics&rdquo;</XREF>). Be careful to ensure that asynchronous operations to a tape are executed in the proper sequence.</PARAGRAPH>
</SECTION2>
<SECTION2 LBL="" HELPID = ""><TITLE><XREFTARGET ID="10662">Generic SCSI Device Driver</TITLE><PARAGRAPH>Generally, non-disk, non-tape SCSI devices are installed in the <INDEXTARGET ID="RPPG_ch0612"><!-- POSTPROCESSDATA: RPPG_ch0612|device driver:generic SCSI --><INDEXTARGET ID="RPPG_ch0613"><!-- POSTPROCESSDATA: RPPG_ch0613|SCSI interface:generic device driver --><FILENAME>/dev/scsi</FILENAME> directory. These devices so named are controlled by the generic SCSI device driver, which is documented in the <REFPAGE>ds(7m)</REFPAGE> reference page.</PARAGRAPH>
<PARAGRAPH>Unlike most kernel-level device drivers, the generic SCSI driver does not support interrupts, and does not support the <FUNCTION>read()</FUNCTION> and <FUNCTION>write()</FUNCTION> functions. Instead, it supports a wide variety of <FUNCTION>ioctl()</FUNCTION> functions that you can use to issue SCSI commands to a device. In order to invoke these operations you prepare a <VARIABLE>dsreq</VARIABLE> structure describing the operation and pass it to the device driver. Operations can include input and output as well as control and diagnostic commands.</PARAGRAPH>
<PARAGRAPH>The programming interface supported by the generic SCSI driver is quite primitive. A library of higher-level functions makes it easier to use. This library is documented in the <INDEXTARGET ID="RPPG_ch0614"><!-- POSTPROCESSDATA: RPPG_ch0614|dslib --><REFPAGE>dslib(3x)</REFPAGE> reference page. It is also described in detail in the <DOCTITLE>IRIX Device Driver Programmer's Guide</DOCTITLE>. The most important functions in it are listed below:</PARAGRAPH>
<BULLETLIST><BULLET><PARAGRAPH><FUNCTION>dsopen()</FUNCTION>, which takes a device pathname, opens it for exclusive access, and returns a <VARIABLE>dsreq</VARIABLE> structure to be used with other functions.</PARAGRAPH>
</BULLET>
<BULLET><PARAGRAPH><FUNCTION>fillg0cmd()</FUNCTION>, <FUNCTION>fillg1cmd()</FUNCTION>, and <FUNCTION>filldsreq()</FUNCTION>, which simplify the task of preparing the many fields of a <VARIABLE>dsreq</VARIABLE> structure for a particular command.</PARAGRAPH>
</BULLET>
<BULLET><PARAGRAPH><FUNCTION>doscsireq()</FUNCTION>, which calls the device driver and checks status afterward.</PARAGRAPH>
</BULLET>
</BULLETLIST>
<PARAGRAPH>The <VARIABLE>dsreq</VARIABLE> structure for some operations specifies a buffer in memory for data transfer. The generic SCSI driver handles the task of locking the buffer into memory (if necessary) and managing a DMA transfer of data.</PARAGRAPH>
<PARAGRAPH>When the <FUNCTION>ioctl()</FUNCTION> function is called (through <FUNCTION>doscsireq()</FUNCTION> or directly), it does not return until the SCSI command is complete. You should only request a SCSI operation from a process that can tolerate being blocked.</PARAGRAPH>
<PARAGRAPH>Built upon the basic dslib functions are several functions that execute specific SCSI commands, for example, <FUNCTION>read08()</FUNCTION> performs a read. However, there are few SCSI commands that are recognized by all devices. Even the read operation has many variations, and the <FUNCTION>read08()</FUNCTION> function as supplied is unlikely to work without modification. The dslib library functions are not complete. Instead, you must alter them and extend them with functions tailored to a specific device.</PARAGRAPH>
<PARAGRAPH>For more on dslib, see the <DOCTITLE>IRIX Device Driver Programmer's Guide</DOCTITLE>.</PARAGRAPH>
</SECTION2>
<SECTION2 LBL="" HELPID = ""><TITLE>CD-ROM and DAT Audio Libraries</TITLE><PARAGRAPH>A library of functions that enable you to read audio data from an audio CD in the CD-ROM drive is distributed with IRIX. This library was built upon the generic SCSI functions supplied in dslib. The CD audio library is documented in the <INDEXTARGET ID="RPPG_ch0615"><!-- POSTPROCESSDATA: RPPG_ch0615|CD-ROM audio library --><REFPAGE>CDintro(3dm)</REFPAGE> reference page (installed with the dmedia_dev package).</PARAGRAPH>
<PARAGRAPH>A library of functions that enable you to read and write audio data from a digital audio tape is distributed with IRIX. This library was built upon the functions of the magnetic tape device driver. The DAT audio library is documented in the <INDEXTARGET ID="RPPG_ch0616"><!-- POSTPROCESSDATA: RPPG_ch0616|DAT audio library --><REFPAGE>DTintro(3dm)</REFPAGE> reference page (installed with the dmedia_dev package). </PARAGRAPH>
</SECTION2>
</SECTION1>
<SECTION1 LBL="" HELPID = ""><TITLE>The PCI Bus</TITLE><PARAGRAPH>Beginning in IRIX 6.5, the PCI Bus Access driver (pciba) can be used on all Silicon Graphics platforms that support PCI for user-level access to the PCI bus and the devices that reside on it. The pciba interface provides a mechanism to access the PCI bus address spaces, handle PCI interrupts, and obtain PCI addresses for DMA from user programs. It provides a convenient mechanism for writing user-level PCI device drivers.<INDEXTARGET ID="RPPG_ch0617"><!-- POSTPROCESSDATA: RPPG_ch0617|PCI bus --></PARAGRAPH>
<PARAGRAPH>The pciba driver is a loadable device driver that is not loaded in the kernel by default. For information on loading the pciba driver see the <REFPAGE>pciba(7M)</REFPAGE> reference page. </PARAGRAPH>
<PARAGRAPH>The pciba driver provides support for<FUNCTION>&space;open()</FUNCTION>, <FUNCTION>close()</FUNCTION>,<FUNCTION>&space;ioctl()</FUNCTION>, and <FUNCTION>mmap()</FUNCTION> functions. It does not support the <FUNCTION>read()</FUNCTION> and <FUNCTION>write()</FUNCTION> driver functions. Using pciba, memory-mapped I/O is performed to PCI address space without the overhead of a system call. PCI bus transactions are transparent to the user. Access to PCI devices is performed by knowing the location of the PCI bus in the hardware graph structure and the slot number where the PCI card resides. Specific information about using the pciba driver can be found in the <REFPAGE>pciba(7M)</REFPAGE> reference page.</PARAGRAPH>
<PARAGRAPH><XREF IDREF="16688" TYPE="TEXT">Example&nbsp;6-1</XREF> shows how to use pciba to map into the memory space of a PCI card on an Origin2000 or Onyx2 system. The code performs an open to the address space found in base register 2 of a PCI device that resides in slot 1 of a PCI shoebox (pci_xio). Then it memory maps 1 MB of memory into the process address space. Lastly, it writes zeros to the first byte of the memory area.</PARAGRAPH>

<!-- WARNING: (47) Example captions should come at end of Example - detected on page 95 -->
<!-- WARNINGLOCATION: PAGE = "95" SRC = "RPPG_ch06.mif" TAGTYPE = "PARA" TAG = "ExampleTitle" TAGCOUNT = "1" UID = "526896" TEXT = " shows how to use pciba t"-->
<EXAMPLE><CAPTION LBL="6-1"><PREFIX>Example 6-1 </PREFIX><XREFTARGET ID="16688">Memory Mapping With pciba</CAPTION></EXAMPLE>
<CODE>
#define PCI40_PATH "/hw/module/1/slot/io2/pci_xio/pci/1/base/2"
#define PCI40_SIZE (1024*1024)

fd = open(PCI40_PATH, O_RDWR);
if (fd &lt; 0 ) {
&space;       perror("open");
&space;       exit (1);
}
pci40_addr = (volatile uchar_t *) mmap(0, PCI40_SIZE,
&space;             PROT_READ|PROT_WRITE,MAP_SHARED, fd, 0);
if (pci40_addr == (uchar_t *) MAP_FAILED) {
&space;       perror("mmap");
&space;       exit (1);
}
pci40_addr= 0x00;
</CODE>
<PARAGRAPH>More information about pciba and user access to the PCI bus on Silicon Graphics systems can be found in the<DOCTITLE>&space;IRIX Device Driver Programming Guide</DOCTITLE>.</PARAGRAPH>
</SECTION1>
<SECTION1 LBL="" HELPID = ""><TITLE><XREFTARGET ID="25650">The VME Bus</TITLE><PARAGRAPH>Each CHALLENGE, Onyx, POWER&nbsp;CHALLENGE, and POWER&nbsp;Onyx system includes full support for the VME interface, including all features of Revision C.2 of the VME specification, and the A64 and D64 modes as defined in Revision D. Each Origin2000, Origin200, and Onyx2 system supports VME as an optional interface.VME devices can access system memory addresses, and devices on the system bus can access addresses in the VME address space.<INDEXTARGET ID="RPPG_ch0618"><!-- POSTPROCESSDATA: RPPG_ch0618|VME bus --></PARAGRAPH>
<PARAGRAPH>The naming of VME devices in <FILENAME>/dev/vme </FILENAME>and<FILENAME>&space;/hw/vme</FILENAME> for Origin2000 systems, and other administrative issues are covered in the <REFPAGE>usrvme(7)</REFPAGE> reference page and the <DOCTITLE>IRIX Device Driver Programming Guide</DOCTITLE>.</PARAGRAPH>
<PARAGRAPH>For information about the physical description of the XIO-VME option for Origin and Onyx2 systems, refer to the <DOCTITLE>Origin2000 and Onyx2 VME Option Owner's Guide</DOCTITLE>.</PARAGRAPH>
<SECTION2 LBL="" HELPID = ""><TITLE>CHALLENGE an Onyx Hardware Nomenclature</TITLE><PARAGRAPH>A number of special terms are used to describe the multiprocessor CHALLENGE support for VME. The terms are described in the following list. Their relationship is shown graphically in <XREF IDREF="56038" TYPE="GRAPHIC">Figure&nbsp;6-1</XREF>. </PARAGRAPH>
<TABLE COLUMNS="2"><TABLEBODY><ROW><CELL LEFT="0" WIDTH="101"><PARAGRAPH>POWERpath-2 Bus</PARAGRAPH>
</CELL>
<CELL LEFT="110" WIDTH="285"><PARAGRAPH>The primary system bus, connecting all CPUs and I/O channels 
to main memory.</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="101"><PARAGRAPH>POWER&nbsp;Channel-2</PARAGRAPH>
</CELL>
<CELL LEFT="110" WIDTH="285"><PARAGRAPH>The circuit card that interfaces one or more I/O devices to the 
POWERpath-2 bus.</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="101"><PARAGRAPH>F-HIO card</PARAGRAPH>
</CELL>
<CELL LEFT="110" WIDTH="285"><PARAGRAPH>Adapter card used for cabling a VME card cage to the 
POWER&nbsp;Channel</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="101"><PARAGRAPH>VMECC</PARAGRAPH>
</CELL>
<CELL LEFT="110" WIDTH="285"><PARAGRAPH>VME control chip, the circuit that interfaces the VME bus to the 
POWER&nbsp;Channel.</PARAGRAPH>
</CELL>
</ROW>
</TABLEBODY>
</TABLE>
<PARAGRAPH><FIGURE><GRAPHIC FILE="datapath.gif" POSITION="INLINE" SCALE="FALSE"><CAPTION LBL="6-1"><PREFIX>Figure 6-1 </PREFIX><XREFTARGET ID="56038">Multiprocessor CHALLENGE Data Path Components</CAPTION>
</FIGURE>
</PARAGRAPH>
</SECTION2>
<SECTION2 LBL="" HELPID = ""><TITLE>VME Bus Attachments</TITLE><PARAGRAPH>All multiprocessor CHALLENGE systems contain a 9U VME bus in the main card cage. Systems configured for rack-mount can optionally include an auxiliary 9U VME card cage, which can be configured as 1, 2, or 4 VME busses. The possible configurations of VME cards are shown in <INDEXTARGET ID="RPPG_ch0619"><!-- POSTPROCESSDATA: RPPG_ch0619|VME bus:configuration --><XREF IDREF="97982" TYPE="TABLE">Table&nbsp;6-1</XREF></PARAGRAPH>
<TABLE COLUMNS="5"><CAPTION LBL="6-1"><PREFIX>Table 6-1 </PREFIX>&space;<EMPHASIS>(continued)        </EMPHASIS><XREFTARGET ID="97982">Multiprocessor CHALLENGE VME Cages and Slots</CAPTION>
<TABLEHEADING><CELL LEFT="0" WIDTH="81"><PARAGRAPH>Model</PARAGRAPH>
</CELL>
<CELL LEFT="90" WIDTH="66"><PARAGRAPH>Main Cage&lbreak;Slots</PARAGRAPH>
</CELL>
<CELL LEFT="165" WIDTH="66"><PARAGRAPH>Aux Cage Slots&lbreak;(1 bus)</PARAGRAPH>
</CELL>
<CELL LEFT="240" WIDTH="66"><PARAGRAPH>Aux Cage Slots&lbreak;(2 busses)</PARAGRAPH>
</CELL>
<CELL LEFT="315" WIDTH="66"><PARAGRAPH>Aux Cage Slots&lbreak;(4 busses)</PARAGRAPH>
</CELL>
</TABLEHEADING>
<TABLEBODY><ROW><CELL LEFT="0" WIDTH="81"><PARAGRAPH>CHALLENGE L</PARAGRAPH>
</CELL>
<CELL LEFT="90" WIDTH="66"><PARAGRAPH>5</PARAGRAPH>
</CELL>
<CELL LEFT="165" WIDTH="66"><PARAGRAPH>n.a.</PARAGRAPH>
</CELL>
<CELL LEFT="240" WIDTH="66"><PARAGRAPH>n.a.</PARAGRAPH>
</CELL>
<CELL LEFT="315" WIDTH="66"><PARAGRAPH>n.a.</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="81"><PARAGRAPH>Onyx Deskside</PARAGRAPH>
</CELL>
<CELL LEFT="90" WIDTH="66"><PARAGRAPH>3</PARAGRAPH>
</CELL>
<CELL LEFT="165" WIDTH="66"><PARAGRAPH>n.a.</PARAGRAPH>
</CELL>
<CELL LEFT="240" WIDTH="66"><PARAGRAPH>n.a.</PARAGRAPH>
</CELL>
<CELL LEFT="315" WIDTH="66"><PARAGRAPH>n.a.</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="81"><PARAGRAPH>CHALLENGE XL</PARAGRAPH>
</CELL>
<CELL LEFT="90" WIDTH="66"><PARAGRAPH>5</PARAGRAPH>
</CELL>
<CELL LEFT="165" WIDTH="66"><PARAGRAPH>20</PARAGRAPH>
</CELL>
<CELL LEFT="240" WIDTH="66"><PARAGRAPH>10 and 9</PARAGRAPH>
</CELL>
<CELL LEFT="315" WIDTH="66"><PARAGRAPH>5, 4, 4, and 4</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="81"><PARAGRAPH>Onyx Rack</PARAGRAPH>
</CELL>
<CELL LEFT="90" WIDTH="66"><PARAGRAPH>4</PARAGRAPH>
</CELL>
<CELL LEFT="165" WIDTH="66"><PARAGRAPH>20</PARAGRAPH>
</CELL>
<CELL LEFT="240" WIDTH="66"><PARAGRAPH>10 and 9</PARAGRAPH>
</CELL>
<CELL LEFT="315" WIDTH="66"><PARAGRAPH>5, 4, 4, and 4</PARAGRAPH>
</CELL>
</ROW>
</TABLEBODY>
</TABLE>
<PARAGRAPH>Each VME bus after the first requires an F cable connection from an F-HIO card on a POWER&nbsp;Channel-2 board, as well as a Remote VCAM board in the auxiliary VME cage. Up to three VME busses (two in the auxiliary cage) can be supported by the first POWER Channel-2 board in a system. A second POWER&nbsp;Channel-2 board must be added to support four or more VME busses. The relationship among VME busses, F-HIO cards, and POWER&nbsp;Channel-2 boards is detailed in <XREF IDREF="47525" TYPE="TABLE">Table&nbsp;6-2</XREF>. </PARAGRAPH>
<TABLE COLUMNS="5"><CAPTION LBL="6-2"><PREFIX>Table 6-2 </PREFIX>&space;<EMPHASIS>(continued)        </EMPHASIS><XREFTARGET ID="47525">POWER&nbsp;Channel-2 and VME bus Configurations</CAPTION>
<TABLEHEADING><CELL LEFT="0" WIDTH="55"><PARAGRAPH>Number of 
VME Busses</PARAGRAPH>
</CELL>
<CELL LEFT="60" WIDTH="74"><PARAGRAPH>PC-2 #1&lbreak;FHIO slot #1</PARAGRAPH>
</CELL>
<CELL LEFT="140" WIDTH="66"><PARAGRAPH>PC-2 #1&lbreak;FHIO slot #2</PARAGRAPH>
</CELL>
<CELL LEFT="215" WIDTH="66"><PARAGRAPH>PC-2 #2&lbreak;FHIO slot #1</PARAGRAPH>
</CELL>
<CELL LEFT="290" WIDTH="66"><PARAGRAPH>PPC-2 #2&lbreak;FHIO slot #2</PARAGRAPH>
</CELL>
</TABLEHEADING>
<TABLEBODY><ROW><CELL LEFT="0" WIDTH="55"><PARAGRAPH>1</PARAGRAPH>
</CELL>
<CELL LEFT="60" WIDTH="74"><PARAGRAPH>unused</PARAGRAPH>
</CELL>
<CELL LEFT="140" WIDTH="66"><PARAGRAPH>unused</PARAGRAPH>
</CELL>
<CELL LEFT="215" WIDTH="66"><PARAGRAPH>n.a.</PARAGRAPH>
</CELL>
<CELL LEFT="290" WIDTH="66"><PARAGRAPH>n.a.</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="55"><PARAGRAPH>2</PARAGRAPH>
</CELL>
<CELL LEFT="60" WIDTH="74"><PARAGRAPH>F-HIO short</PARAGRAPH>
</CELL>
<CELL LEFT="140" WIDTH="66"><PARAGRAPH>unused</PARAGRAPH>
</CELL>
<CELL LEFT="215" WIDTH="66"><PARAGRAPH>n.a.</PARAGRAPH>
</CELL>
<CELL LEFT="290" WIDTH="66"><PARAGRAPH>n.a.</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="55"><PARAGRAPH>3 (1 PC-2)</PARAGRAPH>
</CELL>
<CELL LEFT="60" WIDTH="74"><PARAGRAPH>F-HIO short</PARAGRAPH>
</CELL>
<CELL LEFT="140" WIDTH="66"><PARAGRAPH>F-HIO short</PARAGRAPH>
</CELL>
<CELL LEFT="215" WIDTH="66"><PARAGRAPH>n.a.</PARAGRAPH>
</CELL>
<CELL LEFT="290" WIDTH="66"><PARAGRAPH>n.a.</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="55"><PARAGRAPH>3 (2 PC-2)</PARAGRAPH>
</CELL>
<CELL LEFT="60" WIDTH="74"><PARAGRAPH>unused</PARAGRAPH>
</CELL>
<CELL LEFT="140" WIDTH="66"><PARAGRAPH>unused</PARAGRAPH>
</CELL>
<CELL LEFT="215" WIDTH="66"><PARAGRAPH>F-HIO</PARAGRAPH>
</CELL>
<CELL LEFT="290" WIDTH="66"><PARAGRAPH>unused</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="55"><PARAGRAPH>4</PARAGRAPH>
</CELL>
<CELL LEFT="60" WIDTH="74"><PARAGRAPH>unused</PARAGRAPH>
</CELL>
<CELL LEFT="140" WIDTH="66"><PARAGRAPH>unused</PARAGRAPH>
</CELL>
<CELL LEFT="215" WIDTH="66"><PARAGRAPH>F-HIO</PARAGRAPH>
</CELL>
<CELL LEFT="290" WIDTH="66"><PARAGRAPH>F-HIO</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="55"><PARAGRAPH>5</PARAGRAPH>
</CELL>
<CELL LEFT="60" WIDTH="74"><PARAGRAPH>unused</PARAGRAPH>
</CELL>
<CELL LEFT="140" WIDTH="66"><PARAGRAPH>unused</PARAGRAPH>
</CELL>
<CELL LEFT="215" WIDTH="66"><PARAGRAPH>F-HIO</PARAGRAPH>
</CELL>
<CELL LEFT="290" WIDTH="66"><PARAGRAPH>F-HIO</PARAGRAPH>
</CELL>
</ROW>
</TABLEBODY>
</TABLE>
<PARAGRAPH>F-HIO short cards, which are used only on the first POWER&nbsp;Channel-2 board, supply only one cable output. Regular F-HIO cards, used on the second POWER&nbsp;Channel-2 board, supply two. This explains why, although two POWER&nbsp;Channel-2 boards are needed with four or more VME busses, the F-HIO slots on the first POWER&nbsp;Channel-2 board remain unused.</PARAGRAPH>
</SECTION2>
<SECTION2 LBL="" HELPID = ""><TITLE>VME Address Space Mapping</TITLE><PARAGRAPH>A device on the VME bus has access to an address space in which it can read or write. Depending on the device, it uses 16, 32, or 64 bits to define a bus address. The resulting numbers are called the A16, A32, and A64 address spaces.<INDEXTARGET ID="RPPG_ch0620"><!-- POSTPROCESSDATA: RPPG_ch0620|address space:of VME bus devices --><INDEXTARGET ID="RPPG_ch0621"><!-- POSTPROCESSDATA: RPPG_ch0621|VME bus:address space mapping --></PARAGRAPH>
<PARAGRAPH>There is no direct relationship between an address in the VME address space and the set of real addresses in the system main memory. An address in the VME address space must be translated twice:</PARAGRAPH>
<BULLETLIST><BULLET><PARAGRAPH>The VME interface hardware establishes a translation from VME addresses into addresses in real memory.</PARAGRAPH>
</BULLET>
<BULLET><PARAGRAPH>The IRIX kernel assigns real memory space for this use, and establishes the translation from real memory to virtual memory in the address space of a process or the address space of the kernel.</PARAGRAPH>
</BULLET>
</BULLETLIST>
<PARAGRAPH>Address space mapping is done differently for programmed I/O, in which slave VME devices respond to memory accesses by the program, and for DMA, in which master VME devices read and write directly to main memory.</PARAGRAPH>
<NOTE><PREFIX>Note</PREFIX>VME addressing issues are discussed in greater detail from the standpoint of the device driver, in the <DOCTITLE>IRIX Device Driver Programmer's Guide</DOCTITLE>.</NOTE>
<SECTION3 LBL="" HELPID = ""><TITLE>PIO Address Space Mapping</TITLE><PARAGRAPH>To allow programmed I/O, the <INDEXTARGET ID="RPPG_ch0622"><!-- POSTPROCESSDATA: RPPG_ch0622|PIO address mapping --><INDEXTARGET ID="RPPG_ch0623"><!-- POSTPROCESSDATA: RPPG_ch0623|VME bus:PIO address mapping --><FUNCTION>mmap()</FUNCTION> system function establishes a correspondence between a segment of a process's address space and a segment of the VME address space. The kernel and the VME device driver program registers in the VME bus interface chip and recognizes fetches and stores to specific main memory real addresses and translates them into reads and writes on the VME bus. The devices on the VME bus must react to these reads and writes as slaves; DMA is not supported by this mechanism.</PARAGRAPH>
<PARAGRAPH>For CHALLENGE and Onyx systems, one VME bus interface chip can map as many as 12 different segments of memory. Each segment can be as long as 8&nbsp;MB. The segments can be used singly or in any combination. Thus one VME bus interface chip can support 12 unique mappings of at most 8&nbsp;MB, or a single mapping of 96&nbsp;MB, or combinations between.</PARAGRAPH>
<PARAGRAPH>For systems supporting the XIO-VME option, which uses a Tundra Universe VME interface chip, user-level PIO mapping is allocated as follows:</PARAGRAPH>
<BULLETLIST><BULLET><PARAGRAPH>all A16 and A24 address space is mapped</PARAGRAPH>
</BULLET>
<BULLET><PARAGRAPH>seven additional mappings for a maximum of 512&nbsp;MB in A32 address space</PARAGRAPH>
</BULLET>
</BULLETLIST>
</SECTION3>
<SECTION3 LBL="" HELPID = ""><TITLE><XREFTARGET ID="74488">DMA Mapping</TITLE><PARAGRAPH>DMA mapping is based on the use of page tables stored in system main memory. This allows DMA devices to access the virtual addresses in the address spaces of user processes. The real pages of a DMA buffer can be scattered in main memory, but this is not visible to the DMA device. DMA transfers that span multiple, scattered pages can be performed in a single operation.<INDEXTARGET ID="RPPG_ch0624"><!-- POSTPROCESSDATA: RPPG_ch0624|VME bus:DMA mapping --><INDEXTARGET ID="RPPG_ch0625"><!-- POSTPROCESSDATA: RPPG_ch0625|DMA mapping --></PARAGRAPH>
<PARAGRAPH>The kernel functions that establish the DMA address mapping are available only to device drivers. For information on these, refer to the <DOCTITLE>IRIX Device Driver Programmer's Guide</DOCTITLE>.</PARAGRAPH>
</SECTION3>
</SECTION2>
<SECTION2 LBL="" HELPID = ""><TITLE><XREFTARGET ID="78134">Program Access to the VME Bus</TITLE><PARAGRAPH>Your program accesses the devices on the VME bus in one of two ways, through programmed I/O (PIO) or through DMA. Normally, VME cards with Bus Master capabilities always use DMA, while VME cards with slave capabilities are accessed using PIO.</PARAGRAPH>
<PARAGRAPH>The VME bus interface also contains a unique hardware feature, the DMA Engine, which can be used to move data directly between memory and a slave VME device.</PARAGRAPH>
<SECTION3 LBL="" HELPID = ""><TITLE><XREFTARGET ID="50284">PIO Access</TITLE><PARAGRAPH>Perform PIO to VME devices by mapping the devices into memory using the <INDEXTARGET ID="RPPG_ch0626"><!-- POSTPROCESSDATA: RPPG_ch0626|PIO access to VME devices --><INDEXTARGET ID="RPPG_ch0627"><!-- POSTPROCESSDATA: RPPG_ch0627|VME bus:PIO access --><FUNCTION>mmap()</FUNCTION> function (The use of PIO is covered in greater detail in the <DOCTITLE>IRIX Device Driver Programmer's Guide</DOCTITLE>. Memory mapping of I/O devices and other objects is covered in the book <DOCTITLE>Topics in IRIX Programming</DOCTITLE>.)</PARAGRAPH>
<PARAGRAPH>Each PIO read requires two transfers over the VME bus interface: one to send the address to be read, and one to retrieve the data. The latency of a single PIO input is approximately 4 microseconds on the CHALLENGE/Onyx systems and 2.6 microseconds on the Origin/Onyx2 systems. PIO write is somewhat faster, since the address and data are sent in one operation. Typical PIO performance is summarized in <XREF IDREF="18638" TYPE="TABLE">Table&nbsp;6-3</XREF>.</PARAGRAPH>
<TABLE COLUMNS="5"><CAPTION LBL="6-3"><PREFIX>Table 6-3 </PREFIX>&space;<EMPHASIS>(continued)        </EMPHASIS><XREFTARGET ID="18638">VME Bus PIO Bandwidth <INDEXTARGET ID="RPPG_ch0628"><!-- POSTPROCESSDATA: RPPG_ch0628|VME bus:performance --></CAPTION>
<TABLEHEADING><CELL LEFT="0" WIDTH="63"><PARAGRAPH>Data Unit Size</PARAGRAPH>
</CELL>
<CELL LEFT="70" WIDTH="72"><PARAGRAPH>Reads for 
Origin/Onyx2 
Systems</PARAGRAPH>
</CELL>
<CELL LEFT="150" WIDTH="72"><PARAGRAPH>Reads for 
CHALLENGE/&lbreak;Onyx Systems</PARAGRAPH>
</CELL>
<CELL LEFT="230" WIDTH="72"><PARAGRAPH>Writes for 
Origin/Onyx2 
Systems</PARAGRAPH>
</CELL>
<CELL LEFT="310" WIDTH="108"><PARAGRAPH>Writes for &lbreak;CHALLENGE/&lbreak;Onyx Systems</PARAGRAPH>
</CELL>
</TABLEHEADING>
<TABLEBODY><ROW><CELL LEFT="0" WIDTH="63"><PARAGRAPH>D8</PARAGRAPH>
</CELL>
<CELL LEFT="70" WIDTH="72"><PARAGRAPH>0.35 MB/second</PARAGRAPH>
</CELL>
<CELL LEFT="150" WIDTH="72"><PARAGRAPH>0.2 MB/second</PARAGRAPH>
</CELL>
<CELL LEFT="230" WIDTH="72"><PARAGRAPH>1.5 MB/second</PARAGRAPH>
</CELL>
<CELL LEFT="310" WIDTH="108"><PARAGRAPH>0.75 MB/second</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="63"><PARAGRAPH>D16</PARAGRAPH>
</CELL>
<CELL LEFT="70" WIDTH="72"><PARAGRAPH>0.7 MB/second</PARAGRAPH>
</CELL>
<CELL LEFT="150" WIDTH="72"><PARAGRAPH>0.5 MB/second</PARAGRAPH>
</CELL>
<CELL LEFT="230" WIDTH="72"><PARAGRAPH>3.0 MB/second</PARAGRAPH>
</CELL>
<CELL LEFT="310" WIDTH="108"><PARAGRAPH>1.5 MB/second</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="63"><PARAGRAPH>D32</PARAGRAPH>
</CELL>
<CELL LEFT="70" WIDTH="72"><PARAGRAPH>1.4 MB/second</PARAGRAPH>
</CELL>
<CELL LEFT="150" WIDTH="72"><PARAGRAPH>1 MB/second</PARAGRAPH>
</CELL>
<CELL LEFT="230" WIDTH="72"><PARAGRAPH>6 MB/second</PARAGRAPH>
</CELL>
<CELL LEFT="310" WIDTH="108"><PARAGRAPH>3 MB/second</PARAGRAPH>
</CELL>
</ROW>
</TABLEBODY>
</TABLE>
<PARAGRAPH>When a system has multiple VME buses, you can program concurrent PIO operations from different CPUs to different buses, effectively multiplying the bandwidth by the number of buses. It does not improve performance to program concurrent PIO to a single VME bus.</PARAGRAPH>
<TIP><PREFIX>Tip</PREFIX>When transferring more than 32 bytes of data, you can obtain higher rates using the DMA Engine. See <XREF IDREF="81572" TYPE="TITLE">&ldquo;DMA Engine Access to Slave Devices&rdquo;</XREF>.</TIP>
</SECTION3>
<SECTION3 LBL="" HELPID = ""><TITLE><XREFTARGET ID="16935">User-Level Interrupt Handling</TITLE><PARAGRAPH>If a VME device that you control with PIO can generate interrupts, you can arrange to trap the interrupts in your own program. In this way, you can program the device for some lengthy operation using PIO output to its registers, and then wait until the device returns an interrupt to say the operation is complete.</PARAGRAPH>
<PARAGRAPH>The programming details on user-level interrupts are covered in <XREF IDREF="13812" TYPE="TITLE">Chapter&nbsp;7, &ldquo;Managing User-Level Interrupts.&rdquo;</XREF></PARAGRAPH>
</SECTION3>
<SECTION3 LBL="" HELPID = ""><TITLE><XREFTARGET ID="42290">DMA Access to Master Devices</TITLE><PARAGRAPH>VME bus cards with Bus Master capabilities transfer data using DMA. These transfers are controlled and executed by the circuitry on the VME card. The DMA transfers are directed by the address mapping described under <INDEXTARGET ID="RPPG_ch0629"><!-- POSTPROCESSDATA: RPPG_ch0629|VME bus:DMA to master devices --><INDEXTARGET ID="RPPG_ch0630"><!-- POSTPROCESSDATA: RPPG_ch0630|DMA to VME bus master devices --><XREF IDREF="74488" TYPE="TITLE">&ldquo;DMA Mapping&rdquo;</XREF>.</PARAGRAPH>
<PARAGRAPH>DMA transfers from a Bus Master are always initiated by a kernel-level device driver. In order to exchange data with a VME Bus Master, you open the device and use <INDEXTARGET ID="RPPG_ch0631"><!-- POSTPROCESSDATA: RPPG_ch0631|device driver:for VME bus master --><FUNCTION>read()</FUNCTION> and <FUNCTION>write()</FUNCTION> calls. The device driver sets up the address mapping and initiates the DMA transfers. The calling process is typically blocked until the transfer is complete and the device driver returns.</PARAGRAPH>
<PARAGRAPH>The typical performance of a single DMA transfer is summarized in <INDEXTARGET ID="RPPG_ch0632"><!-- POSTPROCESSDATA: RPPG_ch0632|VME bus:performance --><XREF IDREF="32217" TYPE="TABLE">Table&nbsp;6-4</XREF>. Many factors can affect the performance of DMA, including the characteristics of the device. </PARAGRAPH>
<TABLE COLUMNS="5"><CAPTION LBL="6-4"><PREFIX>Table 6-4 </PREFIX>&space;<EMPHASIS>(continued)        </EMPHASIS><XREFTARGET ID="32217">VME Bus Bandwidth, VME Master Controlling DMA</CAPTION>
<TABLEHEADING><CELL LEFT="0" WIDTH="72"><PARAGRAPH>Data Transfer 
Size</PARAGRAPH>
</CELL>
<CELL LEFT="80" WIDTH="72"><PARAGRAPH>Reads for 
Origin/Onyx2 
Systems</PARAGRAPH>
</CELL>
<CELL LEFT="160" WIDTH="81"><PARAGRAPH>Reads for &lbreak;CHALLENGE/&lbreak;Onyx Systems</PARAGRAPH>
</CELL>
<CELL LEFT="250" WIDTH="81"><PARAGRAPH>Writes for 
Origin/Onyx2 
Systems</PARAGRAPH>
</CELL>
<CELL LEFT="340" WIDTH="81"><PARAGRAPH>Writes for &lbreak;CHALLENGE/&lbreak;Onyx Systems</PARAGRAPH>
</CELL>
</TABLEHEADING>
<TABLEBODY><ROW><CELL LEFT="0" WIDTH="72"><PARAGRAPH>D8</PARAGRAPH>
</CELL>
<CELL LEFT="80" WIDTH="72"><PARAGRAPH>N/A</PARAGRAPH>
</CELL>
<CELL LEFT="160" WIDTH="81"><PARAGRAPH>0.4 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="250" WIDTH="81"><PARAGRAPH>N/A</PARAGRAPH>
</CELL>
<CELL LEFT="340" WIDTH="81"><PARAGRAPH>0.6 MB/sec</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="72"><PARAGRAPH>D16</PARAGRAPH>
</CELL>
<CELL LEFT="80" WIDTH="72"><PARAGRAPH>N/A</PARAGRAPH>
</CELL>
<CELL LEFT="160" WIDTH="81"><PARAGRAPH>0.8 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="250" WIDTH="81"><PARAGRAPH>N/A</PARAGRAPH>
</CELL>
<CELL LEFT="340" WIDTH="81"><PARAGRAPH>1.3 MB/sec</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="72"><PARAGRAPH>D32</PARAGRAPH>
</CELL>
<CELL LEFT="80" WIDTH="72"><PARAGRAPH>N/A</PARAGRAPH>
</CELL>
<CELL LEFT="160" WIDTH="81"><PARAGRAPH>1.6 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="250" WIDTH="81"><PARAGRAPH>N/A</PARAGRAPH>
</CELL>
<CELL LEFT="340" WIDTH="81"><PARAGRAPH>2.6 MB/sec</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="72"><PARAGRAPH>D32 BLOCK</PARAGRAPH>
</CELL>
<CELL LEFT="80" WIDTH="72"><PARAGRAPH>20 MB/sec (256 
byte block)</PARAGRAPH>
</CELL>
<CELL LEFT="160" WIDTH="81"><PARAGRAPH>22 MB/sec (256 
byte block)</PARAGRAPH>
</CELL>
<CELL LEFT="250" WIDTH="81"><PARAGRAPH>24 MB/sec (256 
byte block)</PARAGRAPH>
</CELL>
<CELL LEFT="340" WIDTH="81"><PARAGRAPH>24 MB/sec (256 
byte block)</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="72"><PARAGRAPH>D64 BLOCK</PARAGRAPH>
</CELL>
<CELL LEFT="80" WIDTH="72"><PARAGRAPH>40 MB/sec (2048 
byte block)</PARAGRAPH>
</CELL>
<CELL LEFT="160" WIDTH="81"><PARAGRAPH>55 MB/sec (2048 
byte block)</PARAGRAPH>
</CELL>
<CELL LEFT="250" WIDTH="81"><PARAGRAPH>48 MB/sec (2048 
byte block)</PARAGRAPH>
</CELL>
<CELL LEFT="340" WIDTH="81"><PARAGRAPH>58 MB/sec (2048 
byte block)</PARAGRAPH>
</CELL>
</ROW>
</TABLEBODY>
</TABLE>
</SECTION3>
<SECTION3 LBL="" HELPID = ""><TITLE><XREFTARGET ID="81572">DMA Engine Access to Slave Devices</TITLE><PARAGRAPH>A DMA engine is included as part of, and is unique to each Silicon Graphics VME bus interface. It performs efficient, block-mode, DMA transfers between system memory and VME bus slave cards&mdash;cards that are normally capable of only PIO transfers.<INDEXTARGET ID="RPPG_ch0633"><!-- POSTPROCESSDATA: RPPG_ch0633|DMA engine for VME bus --><INDEXTARGET ID="RPPG_ch0634"><!-- POSTPROCESSDATA: RPPG_ch0634|VME bus:udmalib --><INDEXTARGET ID="RPPG_ch0635"><!-- POSTPROCESSDATA: RPPG_ch0635|udmalib --></PARAGRAPH>
<PARAGRAPH>The DMA engine greatly increases the rate of data transfer compared to PIO, provided that you transfer at least 32 contiguous bytes at a time. The DMA engine can perform D8, D16, D32, D32 Block, and D64 Block data transfers in the A16, A24, and A32 bus address spaces.</PARAGRAPH>
<PARAGRAPH>All DMA engine transfers are initiated by a special device driver. However, you do not access this driver through open/read/write system functions. Instead, you program it through a library of functions. The functions are documented in the <REFPAGE>udmalib(3x)</REFPAGE> (for CHALLENGE/Onyx systems) and the <REFPAGE>vme_dma_engine(3x)</REFPAGE> (for Origin/Onyx2 systems) reference pages. For CHALLENGE/Onyx systems, the functions are used in the following sequence:</PARAGRAPH>
<ORDEREDLIST><LIST><PARAGRAPH>Call <FUNCTION>dma_open()</FUNCTION> to initialize action to a particular VME card.</PARAGRAPH>
</LIST>
<LIST><PARAGRAPH>Call <FUNCTION>dma_allocbuf()</FUNCTION> to allocate storage to use for DMA buffers.</PARAGRAPH>
</LIST>
<LIST><PARAGRAPH>Call <FUNCTION>dma_mkparms()</FUNCTION> to create a descriptor for an operation, including the buffer, the length, and the direction of transfer.</PARAGRAPH>
</LIST>
<LIST><PARAGRAPH>Call <FUNCTION>dma_start()</FUNCTION> to execute a transfer. This function does not return until the transfer is complete.</PARAGRAPH>
</LIST>
</ORDEREDLIST>
<NOTE><PREFIX>Note</PREFIX>The Origin/Onyx2 library also supports these functions, but they are not the preferred interface.</NOTE>
<PARAGRAPH>For the Origin and Onyx2 XIO-VME interface, the VME DMA engine library is used in the following sequence:</PARAGRAPH>
<ORDEREDLIST><LIST><PARAGRAPH>Call <FUNCTION>vme_dma_engine_handle_alloc()</FUNCTION> to allocate a handle for the DMA engine by the given pathname.</PARAGRAPH>
</LIST>
<LIST><PARAGRAPH>Call <FUNCTION>vme_dma_engine_buffer_alloc()</FUNCTION> to allocate the host memory buffer according to the address and byte_count pair.</PARAGRAPH>
</LIST>
<LIST><PARAGRAPH>Call <FUNCTION>vme_dma_engine_transfer_alloc()</FUNCTION> to allocate a transfer entity by the given parameters. Some parameters must be specified, such as the buffer handle, the VME bus address, the number of bytes that are being transferred, the VME bus address space type, and the direction of the transfer. There are two advisory parameters: the throttle size and the release mode.</PARAGRAPH>
</LIST>
<LIST><PARAGRAPH>Call <FUNCTION>vme_dma_engine_schedule()</FUNCTION> to schedule a transfer for the actual DMA action. This call provides a way to schedule multiple transfers for one-time DMA action.</PARAGRAPH>
</LIST>
<LIST><PARAGRAPH>Call <FUNCTION>vme_dma_engine_commit()</FUNCTION> to ask the library to commit all scheduled transfers. Two commitment modes are available: synchronous and asynchronous. </PARAGRAPH>
<BULLETLISTIND><BULLETIND><PARAGRAPH>In synchronous mode, the library returns when the DMA is finished and an advisory parameter specifies the wait method: spin-waiting or sleep-waiting. </PARAGRAPH>
</BULLETIND>
<BULLETIND><PARAGRAPH>In asynchronous mode, the library returns instantly. Call <FUNCTION>vme_dma_engine_rendezvous()</FUNCTION> to wait until all scheduled transfers are complete. Here also are the spin-waiting or sleep-waiting options for waiting.</PARAGRAPH>
</BULLETIND>
</BULLETLISTIND>
</LIST>
</ORDEREDLIST>
<PARAGRAPH>For more details of user DMA, see the <DOCTITLE>IRIX Device Driver Programmer's Guide</DOCTITLE>.</PARAGRAPH>
<PARAGRAPH>The typical performance of the DMA engine for D32 transfers is summarized in <INDEXTARGET ID="RPPG_ch0636"><!-- POSTPROCESSDATA: RPPG_ch0636|VME bus:performance --><INDEXTARGET ID="RPPG_ch0637"><!-- POSTPROCESSDATA: RPPG_ch0637|DMA engine for VME bus:performance --><XREF IDREF="96056" TYPE="TABLE">Table&nbsp;6-5</XREF> and <XREF IDREF="41112" TYPE="TABLE">Table&nbsp;6-6</XREF>. Performance with D64 Block transfers is somewhat less than twice the rate shown in <XREF IDREF="96056" TYPE="TABLE">Table&nbsp;6-5</XREF> and <XREF IDREF="41112" TYPE="TABLE">Table&nbsp;6-6</XREF>. Transfers for larger sizes are faster because the setup time is amortized over a greater number of bytes. </PARAGRAPH>
<TABLE COLUMNS="5"><CAPTION LBL="6-5"><PREFIX>Table 6-5 </PREFIX>&space;<EMPHASIS>(continued)        </EMPHASIS><XREFTARGET ID="96056">VME Bus Bandwidth, DMA Engine, D32 Transfer (CHALLENGE/Onyx Systems)</CAPTION>
<TABLEHEADING><CELL LEFT="0" WIDTH="71"><PARAGRAPH>Transfer Size</PARAGRAPH>
</CELL>
<CELL LEFT="80" WIDTH="66"><PARAGRAPH>Reads</PARAGRAPH>
</CELL>
<CELL LEFT="155" WIDTH="66"><PARAGRAPH>Writes</PARAGRAPH>
</CELL>
<CELL LEFT="230" WIDTH="66"><PARAGRAPH>Block Reads</PARAGRAPH>
</CELL>
<CELL LEFT="305" WIDTH="66"><PARAGRAPH>Block Writes</PARAGRAPH>
</CELL>
</TABLEHEADING>
<TABLEBODY><ROW><CELL LEFT="0" WIDTH="71"><PARAGRAPH>32</PARAGRAPH>
</CELL>
<CELL LEFT="80" WIDTH="66"><PARAGRAPH>2.8 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="155" WIDTH="66"><PARAGRAPH>2.6 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="230" WIDTH="66"><PARAGRAPH>2.7 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="305" WIDTH="66"><PARAGRAPH>2.7 MB/sec</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="71"><PARAGRAPH>64</PARAGRAPH>
</CELL>
<CELL LEFT="80" WIDTH="66"><PARAGRAPH>3.8 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="155" WIDTH="66"><PARAGRAPH>3.8 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="230" WIDTH="66"><PARAGRAPH>4.0 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="305" WIDTH="66"><PARAGRAPH>3.9 MB/sec</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="71"><PARAGRAPH>128</PARAGRAPH>
</CELL>
<CELL LEFT="80" WIDTH="66"><PARAGRAPH>5.0 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="155" WIDTH="66"><PARAGRAPH>5.3 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="230" WIDTH="66"><PARAGRAPH>5.6 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="305" WIDTH="66"><PARAGRAPH>5.8 MB/sec</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="71"><PARAGRAPH>256</PARAGRAPH>
</CELL>
<CELL LEFT="80" WIDTH="66"><PARAGRAPH>6.0 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="155" WIDTH="66"><PARAGRAPH>6.7 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="230" WIDTH="66"><PARAGRAPH>6.4 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="305" WIDTH="66"><PARAGRAPH>7.3 MB/sec</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="71"><PARAGRAPH>512</PARAGRAPH>
</CELL>
<CELL LEFT="80" WIDTH="66"><PARAGRAPH>6.4 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="155" WIDTH="66"><PARAGRAPH>7.7 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="230" WIDTH="66"><PARAGRAPH>7.0 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="305" WIDTH="66"><PARAGRAPH>8.0 MB/sec</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="71"><PARAGRAPH>1024</PARAGRAPH>
</CELL>
<CELL LEFT="80" WIDTH="66"><PARAGRAPH>6.8 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="155" WIDTH="66"><PARAGRAPH>8.0 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="230" WIDTH="66"><PARAGRAPH>7.5 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="305" WIDTH="66"><PARAGRAPH>8.8 MB/sec</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="71"><PARAGRAPH>2048</PARAGRAPH>
</CELL>
<CELL LEFT="80" WIDTH="66"><PARAGRAPH>7.0 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="155" WIDTH="66"><PARAGRAPH>8.4 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="230" WIDTH="66"><PARAGRAPH>7.8 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="305" WIDTH="66"><PARAGRAPH>9.2 MB/sec</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="71"><PARAGRAPH>4096</PARAGRAPH>
</CELL>
<CELL LEFT="80" WIDTH="66"><PARAGRAPH>7.1 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="155" WIDTH="66"><PARAGRAPH>8.7 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="230" WIDTH="66"><PARAGRAPH>7.9 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="305" WIDTH="66"><PARAGRAPH>9.4 MB/sec</PARAGRAPH>
</CELL>
</ROW>
</TABLEBODY>
</TABLE>
<TABLE COLUMNS="5"><CAPTION LBL="6-6"><PREFIX>Table 6-6 </PREFIX>&space;<EMPHASIS>(continued)        </EMPHASIS><XREFTARGET ID="41112">VME Bus Bandwidth, DMA Engine, D32 Transfer (Origin/Onyx2 Systems)</CAPTION>
<TABLEHEADING><CELL LEFT="0" WIDTH="71"><PARAGRAPH>Transfer Size</PARAGRAPH>
</CELL>
<CELL LEFT="80" WIDTH="66"><PARAGRAPH>Reads</PARAGRAPH>
</CELL>
<CELL LEFT="155" WIDTH="66"><PARAGRAPH>Writes</PARAGRAPH>
</CELL>
<CELL LEFT="230" WIDTH="66"><PARAGRAPH>Block Reads</PARAGRAPH>
</CELL>
<CELL LEFT="305" WIDTH="66"><PARAGRAPH>Block Writes</PARAGRAPH>
</CELL>
</TABLEHEADING>
<TABLEBODY><ROW><CELL LEFT="0" WIDTH="71"><PARAGRAPH>32</PARAGRAPH>
</CELL>
<CELL LEFT="80" WIDTH="66"><PARAGRAPH>1.2 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="155" WIDTH="66"><PARAGRAPH>1.1 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="230" WIDTH="66"><PARAGRAPH>1.2 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="305" WIDTH="66"><PARAGRAPH>1.2 MB/sec</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="71"><PARAGRAPH>64</PARAGRAPH>
</CELL>
<CELL LEFT="80" WIDTH="66"><PARAGRAPH>2.0 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="155" WIDTH="66"><PARAGRAPH>1.9 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="230" WIDTH="66"><PARAGRAPH>2.0 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="305" WIDTH="66"><PARAGRAPH>2.0 MB/sec</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="71"><PARAGRAPH>128</PARAGRAPH>
</CELL>
<CELL LEFT="80" WIDTH="66"><PARAGRAPH>3.3 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="155" WIDTH="66"><PARAGRAPH>3.5 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="230" WIDTH="66"><PARAGRAPH>3.3 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="305" WIDTH="66"><PARAGRAPH>3.9 MB/sec</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="71"><PARAGRAPH>256</PARAGRAPH>
</CELL>
<CELL LEFT="80" WIDTH="66"><PARAGRAPH>5.1 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="155" WIDTH="66"><PARAGRAPH>5.6 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="230" WIDTH="66"><PARAGRAPH>5.2 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="305" WIDTH="66"><PARAGRAPH>6.3 MB/sec</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="71"><PARAGRAPH>512</PARAGRAPH>
</CELL>
<CELL LEFT="80" WIDTH="66"><PARAGRAPH>6.9 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="155" WIDTH="66"><PARAGRAPH>8.2 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="230" WIDTH="66"><PARAGRAPH>7.3 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="305" WIDTH="66"><PARAGRAPH>9.0 MB/sec</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="71"><PARAGRAPH>1024</PARAGRAPH>
</CELL>
<CELL LEFT="80" WIDTH="66"><PARAGRAPH>8.0 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="155" WIDTH="66"><PARAGRAPH>10.5 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="230" WIDTH="66"><PARAGRAPH>8.8 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="305" WIDTH="66"><PARAGRAPH>12.0 MB/sec</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="71"><PARAGRAPH>2048</PARAGRAPH>
</CELL>
<CELL LEFT="80" WIDTH="66"><PARAGRAPH>9.2 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="155" WIDTH="66"><PARAGRAPH>12.2 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="230" WIDTH="66"><PARAGRAPH>9.8 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="305" WIDTH="66"><PARAGRAPH>14.0 MB/sec</PARAGRAPH>
</CELL>
</ROW>
<ROW><CELL LEFT="0" WIDTH="71"><PARAGRAPH>4096</PARAGRAPH>
</CELL>
<CELL LEFT="80" WIDTH="66"><PARAGRAPH>9.6 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="155" WIDTH="66"><PARAGRAPH>12.6 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="230" WIDTH="66"><PARAGRAPH>11.3 MB/sec</PARAGRAPH>
</CELL>
<CELL LEFT="305" WIDTH="66"><PARAGRAPH>15.1 MB/sec</PARAGRAPH>
</CELL>
</ROW>
</TABLEBODY>
</TABLE>
<PARAGRAPH>Some of the factors that affect the performance of user DMA include</PARAGRAPH>
<BULLETLIST><BULLET><PARAGRAPH>The response time of the VME board to bus read and write requests</PARAGRAPH>
</BULLET>
<BULLET><PARAGRAPH>The size of the data block transferred (as shown in <XREF IDREF="96056" TYPE="TABLE">Table&nbsp;6-5</XREF>)</PARAGRAPH>
</BULLET>
<BULLET><PARAGRAPH>Overhead and delays in setting up each transfer</PARAGRAPH>
</BULLET>
</BULLETLIST>
<PARAGRAPH>The numbers in <XREF IDREF="96056" TYPE="TABLE">Table&nbsp;6-5</XREF> were achieved by a program that called <FUNCTION>dma_start()</FUNCTION> in a tight loop, in other words, with minimal overhead.</PARAGRAPH>
<PARAGRAPH>The <FUNCTION>dma_start()</FUNCTION> and <FUNCTION>vme_dma_engine_commit()</FUNCTION> functions operate in user space; they are not kernel-level device driver calls. This has two important effects. First, overhead is reduced, since there are no mode switches between user and kernel, as there are for <FUNCTION>read()</FUNCTION> and <FUNCTION>write()</FUNCTION>. This is important since the DMA engine is often used for frequent, small inputs and outputs.</PARAGRAPH>
<PARAGRAPH>Second, <FUNCTION>dma_start()</FUNCTION> does not block the calling process, in the sense of suspending it and possibly allowing another process to use the CPU. However, it waits in a test loop, polling the hardware until the operation is complete. As you can infer from <XREF IDREF="96056" TYPE="TABLE">Table&nbsp;6-5</XREF>, typical transfer times range from 50 to 250 microseconds. You can calculate the approximate duration of a call to <FUNCTION>dma_start()</FUNCTION> based on the amount of data and the operational mode.</PARAGRAPH>
<PARAGRAPH>The <FUNCTION>vme_dma_engine_commit()</FUNCTION> call can be used either synchronously (as described for the <FUNCTION>dma_start()</FUNCTION> library call) or asynchronously. If the call is made asynchronously, the transfer completes (in parallel) while the process continues to execute. Because of this, the user process must coordinate with DMA completion using the <FUNCTION>vme_dma_engine_rendezvous()</FUNCTION> call.</PARAGRAPH>
<PARAGRAPH>You can use the udmalib functions to access a VME Bus Master device, if the device can respond in slave mode. However, this may be less efficient than using the Master device's own DMA circuitry.</PARAGRAPH>
<PARAGRAPH>While you can initiate only one DMA engine transfer per bus, it is possible to program a DMA engine transfer from each bus in the system, concurrently. </PARAGRAPH>
</SECTION3>
</SECTION2>
</SECTION1>
<SECTION1 LBL="" HELPID = ""><TITLE><XREFTARGET ID="73027">Serial Ports</TITLE><PARAGRAPH>IRIX 6.5 adds support for the user mode serial library, or usio, which provides access to the system serial ports on Origin, O2, and OCTANE systems, without the overhead of system calls. On these systems, the device <FILENAME>/dev/ttyus</FILENAME><VARIABLE>*</VARIABLE> is mapped into the user process's address space and is accessed directly by the library routines. The user mode library provides read, write, and error detection routines. In addition to the library routines, ioctl support is provided to perform functions that are not time critical, such as port configuration. The <FUNCTION>read()</FUNCTION> and <FUNCTION>write()</FUNCTION> system calls are not supported for this device type, as these functions are implemented in the user library. For complete information about usio, see the <REFPAGE>usio(7)</REFPAGE> reference page.</PARAGRAPH>
<PARAGRAPH>On the Origin, O2, and OCTANE systems, support for a character-based interface on the serial ports is also provided as a low-cost alternative for applications needing bulk data transfer with no character interpretation, via the serial ports. For more information, see the <REFPAGE>cserialio(7)</REFPAGE> reference page.</PARAGRAPH>
<PARAGRAPH>Systems that do not support usio or cserialio must rely on the serial device drivers and STREAMS modules for an input device that interfaces through a serial port for real-time programs. This is not a recommended practice for several reasons: the serial device drivers and the STREAMS modules that process serial input are not optimized for deterministic, real-time performance; and at high data rates, serial devices generate many interrupts.</PARAGRAPH>
<PARAGRAPH>When there is no alternative, a real-time program will typically open one of the files named <FILENAME>/dev/tty*</FILENAME>. The names, and some hardware details, for these devices are documented in the <REFPAGE>serial(7)</REFPAGE> reference page. Information specific to two serial adapter boards is in the <REFPAGE>duart(7)</REFPAGE> reference page and the <REFPAGE>cdsio(7)</REFPAGE> reference page.</PARAGRAPH>
<PARAGRAPH>When a process opens a serial device, a line discipline STREAMS module is pushed on the stream by default. If the real-time device is not a terminal and doesn't support the usual line controls, this module can be removed. Use the I_POP ioctl (see the <REFPAGE>streamio(7)</REFPAGE> reference page) until no modules are left on the stream. This minimizes the overhead of serial input, at the cost of receiving completely raw, unprocessed input.</PARAGRAPH>
<PARAGRAPH>An important feature of current device drivers for serial ports is that they try to minimize the overhead of handling the many interrupts that result from high character data rates. The serial I/O boards interrupt at least every 4 bytes received, and in some cases on every character (at least 480 interrupts a second, and possibly 1920, at 19,200 bps). Rather than sending each input byte up the stream as it arrives, the drivers buffer a few characters and send multiple characters up the stream.</PARAGRAPH>
<PARAGRAPH>When the line discipline module is present on the stream, this behavior is controlled by the <VARIABLE>termio</VARIABLE> settings, as described in the <REFPAGE>termio(7)</REFPAGE> reference page for non-canonical input. However, a real-time program will probably not use the line-discipline module. The hardware device drivers support the SIOC_ITIMER ioctl that is mentioned in the <REFPAGE>serial(7)</REFPAGE> reference page, for the same purpose.</PARAGRAPH>
<PARAGRAPH>The SIOC_ITIMER function specifies the number of clock ticks (see <XREF IDREF="52834" TYPE="TITLE">&ldquo;Tick Interrupts&rdquo;</XREF>) over which it should accumulate input characters before sending a batch of characters up the input stream. A value of 0 requests that each character be sent as it arrives (do this only for devices with very low data rates, or when it is absolutely necessary to know the arrival time of each input byte). A value of 5 tells the driver to collect input for 5 ticks (50 milliseconds, or as many as 24 bytes at 19,200 bps) before passing the data along.</PARAGRAPH>
</SECTION1>
<SECTION1 LBL="" HELPID = ""><TITLE><XREFTARGET ID="78160">External Interrupts</TITLE><PARAGRAPH>The Origin, CHALLENGE, Onyx, and Onyx2 systems include support for generating and receiving external interrupt signals. The electrical interface to the external interrupt lines is documented in the <INDEXTARGET ID="RPPG_ch0638"><!-- POSTPROCESSDATA: RPPG_ch0638|external interrupt --><REFPAGE>ei(7)</REFPAGE> reference page.</PARAGRAPH>
<PARAGRAPH>Your program controls and receives external interrupts by interacting with the external interrupt device driver. This driver is associated with the special device file <FILENAME>/dev/ei</FILENAME>, and is documented in the <REFPAGE>ei(7)</REFPAGE> reference page.&space;<INDEXTARGET ID="RPPG_ch0639"><!-- POSTPROCESSDATA: RPPG_ch0639|<ITALICS>/dev/ei</ITALICS> --></PARAGRAPH>
<PARAGRAPH>For programming details of the external interrupt lines, see the <DOCTITLE>IRIX Device Driver Programmer's Guide</DOCTITLE>. You can also trap external interrupts with a user-level interrupt handler (see <XREF IDREF="16935" TYPE="TITLE">&ldquo;User-Level Interrupt Handling&rdquo;</XREF>); this is also covered in the <DOCTITLE>IRIX Device Driver Programmer's Guide</DOCTITLE>.</PARAGRAPH>
</SECTION1>
</CHAPTER>
